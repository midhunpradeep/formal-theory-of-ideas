<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="Midhun Pradeep Nair, ChatGPT-4o" />
  <meta name="dcterms.date" content="2025-04-13" />
  <title>formal-theory-of-ideas</title>
  <style>
    html {
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 12px;
      }
      h1 {
        font-size: 1.8em;
      }
    }
    @media print {
      html {
        background-color: white;
      }
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: #1a1a1a;
    }
    a:visited {
      color: #1a1a1a;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, Consolas, 'Lucida Console', monospace;
      font-size: 85%;
      margin: 0;
      hyphens: manual;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC ul {
      padding-left: 1.3em;
    }
    #TOC > ul {
      padding-left: 0;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
</head>
<body>
<header id="title-block-header">
<h1 class="title">formal-theory-of-ideas</h1>
<p class="author">Midhun Pradeep Nair, ChatGPT-4o</p>
<p class="date">April 13, 2025</p>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#abstract" id="toc-abstract">Abstract</a></li>
<li><a href="#introduction" id="toc-introduction">Introduction</a></li>
<li><a href="#version-1-primitive-and-unary-abstraction"
id="toc-version-1-primitive-and-unary-abstraction">Version 1 – Primitive
and Unary Abstraction</a></li>
<li><a href="#version-2-multi-modal-unary-abstraction"
id="toc-version-2-multi-modal-unary-abstraction">Version 2 – Multi-Modal
Unary Abstraction</a></li>
<li><a href="#version-3-functional-binary-abstraction"
id="toc-version-3-functional-binary-abstraction">Version 3 – Functional
Binary Abstraction</a></li>
<li><a
href="#version-4-minimal-binary-abstraction-with-pattern-driven-semantics"
id="toc-version-4-minimal-binary-abstraction-with-pattern-driven-semantics">Version
4 – Minimal Binary Abstraction with Pattern-Driven Semantics</a></li>
<li><a href="#philosophical-meaning"
id="toc-philosophical-meaning">Philosophical Meaning</a></li>
<li><a href="#applications" id="toc-applications">Applications</a></li>
<li><a href="#conclusion" id="toc-conclusion">Conclusion</a></li>
</ul>
</nav>
<h2 id="abstract">Abstract</h2>
<p>We develop a rigorous formal theory of <strong>ideas</strong> built
from a single primitive element via successive levels of abstraction.
Starting from one fundamental primitive, we construct a hierarchy of
abstraction systems culminating in a <strong>minimal binary
abstraction</strong> framework. Each version of the theory (from a unary
abstraction to a multi-modal extension, then to functional binary
abstraction, and finally to a minimal binary system with pattern-based
semantics) is defined with formal axioms, definitions, and theorems. We
assume <strong>only structural identity</strong> of ideas – two ideas
are identical if and only if their structural construction is identical
– while deferring any notion of semantic equivalence to future research.
A core result is that <em>abstraction chains are unique</em>: each idea
can be generated by a unique sequence of abstractions, yielding a
canonical form. We introduce a <strong>δ-based complexity
measure</strong> that uniquely encodes the generative structure of each
idea, ensuring that complexity values distinguish distinct ideas.
Importantly, the framework <strong>permits conceptual nonsense</strong>
– structurally well-formed but semantically meaningless combinations –
as legitimate formal ideas, with justification for why including such
nonsensical constructs is both necessary and illuminating. We draw
connections to structuralist and constructivist philosophies, comparing
our approach to set theory, type theory, and category theory. We examine
implications for cognition (how thoughts can be composed),
self-reference (how an idea might reference or apply to itself), and
even the role of nonsense in thought. The tone throughout is rigorous
and methodical: we carefully justify each step, highlight limitations at
each stage, and remain appropriately skeptical about speculative
interpretations. In closing, we discuss potential applications of this
theory in knowledge representation, formal ontology, and the analysis of
conceptual systems, while acknowledging that full semantic
interpretation remains an open challenge.</p>
<h2 id="introduction">Introduction</h2>
<p>What is the <strong>structure of an idea</strong>? Can we formalize
ideas in the way mathematicians formalize numbers or sets, starting from
first principles? This paper proposes a foundational theory in which all
<strong>ideas</strong> are built from a <strong>single
primitive</strong> through iterated abstraction operations. By “idea,”
we mean a unit of thought or concept, stripped of any presumed meaning
and considered only in terms of its structural relations to other ideas.
The motivation is analogous to foundational work in mathematics: just as
Zermelo-Fraenkel set theory builds all sets from the empty set ∅ via
membership, and just as arithmetic builds all numbers from 0 via
successor, we aim to build all ideas from a unique primitive via
<strong>abstraction</strong>.</p>
<p>Our approach is <strong>formal and structural</strong>. We make
<strong>no assumptions about intrinsic meaning</strong> – in fact, we
explicitly <em>defer semantic equivalence</em> to future research. In
this theory, two ideas are considered the <em>same</em> only if they
have the same structure of construction. This principle is akin to the
<em>axiom of extensionality</em> in set theory, which states that a set
is completely determined by its elements (<a
href="https://proofwiki.org/wiki/Axiom:Axiom_of_Extension#:~:text=The%20Axiom%20of%20Extension%20is,determined%20by%20its%20%205">Axiom:Axiom
of Extension - ProofWiki</a>) (<a
href="https://proofwiki.org/wiki/Axiom:Axiom_of_Extension#:~:text=%24A%24%20and%20%24B%24%20are%20equal">Axiom:Axiom
of Extension - ProofWiki</a>). Here, an idea is completely determined by
how it is constructed from the primitive via abstraction steps. We do
not collapse or identify ideas based on any external interpretation or
semantic content, only by their formal construction. This yields a
purely <strong>structural identity criterion</strong>: ideas are
<em>identical iff their construction tree is identical</em>.</p>
<p>Because we focus on structural form, our theory can be seen as an
extreme form of <strong>structuralism</strong> applied to concepts. In
the structuralist view (originating in linguistics and anthropology),
elements gain identity through their relations rather than any intrinsic
content (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=For%20Saussure%2C%20there%20are%20no,arbor%2Ftree%2Fsound%20image%2Fsignifier%20vs">Untitled
Document</a>). Saussure famously said <em>“in language there are only
differences without positive terms”</em> (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=For%20Saussure%2C%20there%20are%20no,arbor%2Ftree%2Fsound%20image%2Fsignifier%20vs">Untitled
Document</a>), meaning that concepts (signifieds) and words (signifiers)
are defined by their differences and relations within a structure, not
by standalone meaning. Our formal ideas likewise have no “positive”
content aside from their place in an abstract construction network. Any
meaning they may eventually acquire would come from relations or
interpretations external to the formal system. In a similar vein,
<strong>post-structuralist</strong> thought (Derrida’s concept of
<em>différance</em>, for example) holds that meaning is always deferred
and arises from a web of differences (<a
href="https://en.wikipedia.org/wiki/Diff%C3%A9rance#:~:text=Diff%C3%A9rance%20is%20a%20French%20,relationship%20between%20text%20and%20meaning">Différance
- Wikipedia</a>). Our stance resonates with this: <strong>semantic
equivalence is deferred</strong> – we will not declare two
differently-constructed ideas “the same meaning” even if an external
observer might interpret them similarly. Instead, we leave the issue of
semantic mapping to future work, focusing here on the <em>generation and
structure</em> of ideas, not their interpretation.</p>
<p>This approach also aligns with a formalist philosophy of mathematics
exemplified by David Hilbert, who viewed mathematics as a symbolic game
played with marks on paper devoid of intrinsic meaning (<a
href="https://libquotes.com/david-hilbert#:~:text=Mathematics,meaningless%20marks%20on%20a%20paper">David
Hilbert (40+ Sourced Quotes) - Lib Quotes</a>). By analogy, our theory
treats ideas as formal symbols (generated by rules) and manipulates them
according to those rules. <strong>Meaninglessness (in a semantic sense)
is not only permitted but expected</strong> at the formal level. This
extends even to what one might call “nonsense” ideas – constructions
that, if interpreted, might yield paradox or meaninglessness. In our
formal system, such <em>conceptual nonsense is allowed and in fact
formally representable</em>. We will argue that allowing nonsensical
combinations (rather than forbidding them via type constraints or
well-formedness restrictions) is beneficial: it gives the theory
completeness in expression and mirrors certain aspects of creative
thought and philosophical inquiry, where exploring beyond the bounds of
immediate sense can be illuminating.</p>
<p>From a constructive standpoint, our theory is <strong>built step by
step</strong> – it is inherently <em>constructivist</em>. We literally
construct the universe of all possible ideas from nothing but one
primitive. This echoes the constructivist epistemology (à la Piaget)
where knowledge is built from primitive cognitive operations, and also
<em>mathematical constructivism</em> where one builds mathematical
objects from basic ones rather than assuming their existence. In our
case, we assume the existence of a single undefined <strong>primitive
idea</strong> (think of it as an “ur-idea” or an empty placeholder of
thought) and an initial abstraction operation. Everything else must be
obtained by applying allowable operations in sequence. As we progress,
we will enrich the abstraction mechanism to increase expressive
power:</p>
<ul>
<li><p><strong>Version 1 (Primitive and Unary Abstraction):</strong> We
start with the simplest system: one primitive idea and a single unary
abstraction operator. This produces a linear “chain” of ideas <span
class="math inline"><em>I</em><sub>0</sub>, <em>δ</em>(<em>I</em><sub>0</sub>), <em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>)), …</span>
ad infinitum. We formalize this and prove that even in this simple
system, each idea has a unique construction sequence. However, the
expressive power is extremely limited (essentially just an infinite
sequence with no branching).</p></li>
<li><p><strong>Version 2 (Multi-Modal Unary Abstraction):</strong> We
extend the unary abstraction to multiple <em>modes</em>. Instead of one
abstraction operation, we have a variety (possibly an infinite set) of
unary operators, each representing a different “mode” or kind of
abstraction. This yields a branching tree of ideas: from the primitive,
one can abstract in mode A or mode B, etc., and then further abstract
those results in various modes, and so on. We will see that the
<strong>abstraction chains remain unique</strong> in this richer
setting; essentially, each idea can be identified with a unique sequence
of mode choices. This version is more expressive (able to produce a
diverse tree of ideas), but still lacks any mechanism to
<em>combine</em> two independent ideas – each new idea stems from
exactly one prior idea.</p></li>
<li><p><strong>Version 3 (Functional Binary Abstraction):</strong> Here
we introduce a fundamentally new operation: a <strong>binary
abstraction</strong> that allows one idea to be applied to another, in a
functional manner. In other words, any idea can now play the role of a
<em>function</em> (an abstract operator) and be applied to any other
idea (as an argument) to form a new idea. Formally, we introduce a
binary constructor <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> meaning
“the abstraction of <span class="math inline"><em>Y</em></span> under
<span class="math inline"><em>X</em></span>” (or “idea <span
class="math inline"><em>X</em></span> applied to idea <span
class="math inline"><em>Y</em></span>”). This dramatically increases the
generative complexity: we can now combine separate branches of the
abstraction tree, not just extend one branch. Version 3 thus subsumes
the previous versions – for example, a unary mode from Version 2 can be
represented as a special idea <span
class="math inline"><em>F</em></span> that when applied to <span
class="math inline"><em>X</em></span> (i.e. <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>) mimics
that mode of abstraction. The <strong>function-argument
structure</strong> of <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> also
introduces <em>ordered pair</em> structures into our idea system. We
show that, assuming no additional identifications (i.e. <span
class="math inline"><em>δ</em></span> is treated as a <em>free</em>
binary constructor), the uniqueness of abstraction representation still
holds: each idea now has a unique construction tree (a rooted ordered
tree) rather than just a chain. We also define a preliminary
<strong>complexity measure</strong> – essentially a count or code based
on the number of abstraction steps – and argue that it <strong>uniquely
determines the structure</strong> of an idea. Intuitively, because each
idea has a unique build sequence, one can encode this sequence (for
instance, as a Gödel number or a binary string), and that encoding will
be one-to-one with the idea itself. In formal terms, there exists an
injective function <span
class="math inline"><em>δ</em><sup>*</sup></span> (not to be confused
with the operator <span class="math inline"><em>δ</em></span>) mapping
each idea to a natural number that represents its <em>complexity</em>.
This numeric “complexity” is <em>generative</em>: it increases with each
abstraction applied, and no two distinct ideas share the same complexity
value. We will give a theorem to this effect. Version 3 is the first
version in which <strong>self-reference</strong> becomes possible in a
non-trivial way – an idea could be applied to itself, <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span>, without
violating any formation rule (just as non-well-founded set theory
permits <span class="math inline"><em>x</em> ∈ <em>x</em></span> (<a
href="https://plato.stanford.edu/entries/nonwellfounded-set-theory/#:~:text=The%20term%20non,Foundation%20Axiom">Non-wellfounded
Set Theory (Stanford Encyclopedia of Philosophy)</a>)). Such an idea has
a perfectly valid structural identity in our system (a node pointing to
itself as both function and argument), even if it might seem
semantically puzzling. This is an example of <em>conceptual nonsense
that our theory allows</em>: a structure that classical type theory or
well-founded set theory would ban can exist here. We discuss the
ramifications of this and why allowing it does not produce inconsistency
(since we have no global axioms like Foundation that it would violate),
but instead provides a broader canvas to study the limits of
abstraction.</p></li>
<li><p><strong>Version 4 (Minimal Binary Abstraction with Pattern-Driven
Semantics):</strong> In the final version, we push the system to an
extreme of simplicity: we reduce the generative basis to the
<em>minimum</em>. In practice, Version 4 has <strong>one single
primitive idea</strong> (the same one we started with) and <strong>one
single binary abstraction operator</strong> <span
class="math inline"><em>δ</em>(⋅,⋅)</span> with no further distinctions.
All the variety of idea constructions must now be encoded <em>within the
binary structure itself</em>. For example, if in Version 2 we had modes
A, B, C, now there are no named modes – we might instead represent what
was “mode A applied to X” as some binary pattern <span
class="math inline"><em>δ</em>(<em>P</em>,<em>X</em>)</span> where <span
class="math inline"><em>P</em></span> is the primitive idea in a
particular position, or some fixed pattern of <span
class="math inline"><em>δ</em></span> involving <span
class="math inline"><em>P</em></span> that signifies that mode. The
semantics in this version, to the extent we discuss semantics at all,
will be <strong>pattern-driven</strong>: certain structural patterns of
the binary tree are interpreted as corresponding to what were distinct
abstraction modalities or meanings. Since the formal system still has no
built-in semantics, this pattern interpretation is an external
commentary – a glimpse at how one <em>might</em> assign meaning later.
We will emphasize that within the formal theory, pattern recognition is
the only guide to meaning: e.g., an idea of the form <span
class="math inline"><em>δ</em>(<em>P</em>,<em>X</em>)</span> might be
given an interpretation (by an outside observer) distinct from <span
class="math inline"><em>δ</em>(<em>X</em>,<em>P</em>)</span>, simply
because of the structural position of <span
class="math inline"><em>P</em></span>. Version 4 essentially presents
the <strong>universal structure of a rooted ordered tree with a unique
atom at the leaves</strong> (since any structure is composed of the
primitive <span class="math inline"><em>P</em></span> combined via <span
class="math inline"><em>δ</em></span> nodes). It is a minimalist
<em>algebra of ideas</em> with just one generator and one binary
operation. Despite (or rather, due to) its minimalism, this version is
the most <em>powerful and most general</em>. We will show that any idea
from Version 3 (with specific functional abstractions) can be encoded as
a structure in Version 4. Moreover, the uniqueness of abstraction chains
and the injective complexity measure carry over – essentially because we
have boiled the system down to an <strong>initial algebra</strong> on a
single generator that yields unique (up to isomorphism) representations
(<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=signature%2C%20and%20this%20object%2C%20unique,5">Term
algebra - Wikipedia</a>). In this version, <strong>conceptual nonsense
is not just allowed but abundant</strong> – without any restrictive
typing or mode constraints, <em>any</em> binary combination of ideas (no
matter how absurd it might seem conceptually) is a valid idea. We regard
this as a feature: it forces us to confront the difference between
<em>structural possibility</em> and <em>semantic plausibility</em>. The
theory, being formal, catalogues all structural possibilities; actual
meaningful concepts would be a (possibly small) subset of those,
filtered by whatever semantic or pragmatic constraints operate in the
mind or world. In philosophical terms, Version 4’s free-wheeling
generativity echoes the post-structuralist idea of a play of signifiers
unbounded by fixed meaning – a kind of unlimited semiosis where even
“nonsense” signifier chains are part of the structure of
thought.</p></li>
</ul>
<p>Following the development of these four versions, we include a
section on the <strong>Philosophical Meaning</strong> of the theory,
where we discuss how this formal framework connects to broader
philosophical ideas. We will draw parallels to
<strong>structuralism</strong> (as mentioned, form over content),
<strong>constructivism</strong> (building a world of ideas from
scratch), <strong>phenomenology</strong> (the structures of
consciousness and experience, possibly relating our primitive to a raw
unanalyzed experience and abstraction to the act of intentionality or
constitution of objects in consciousness), and
<strong>post-structuralism</strong> (the inevitability of nonsense and
the deferral of meaning). We will also examine intriguing implications
for <strong>thought and cognition</strong>: for instance, if every
complex thought corresponds to a unique abstraction chain, this
resonates with the idea of a mental <em>language of thought</em> (LOT)
where complex concepts have a combinatorial structure. Our theory
guarantees uniqueness of such structure, which could relate to the
cognitive notion of <em>canonical representations</em> of concepts. The
role of <strong>type theory</strong> is also discussed: type theory in
computer science and logic imposes restrictions to rule out nonsensical
programs or statements (e.g. preventing one from adding a number to a
truth value, which would be rejected as “nonsense” (<a
href="https://carloangiuli.com/courses/b619-sp24/notes.pdf#:~:text=type%02checker%20may%20reject%20as%20nonsense,scoped">Principles
of Dependent Type Theory</a>)). Our untyped approach deliberately
relinquishes these restrictions to study the full space of expressions;
we compare the pros and cons of these stances.
<strong>Self-reference</strong> and paradoxes are touched upon: we
consider how our system can represent self-referential ideas (which
traditional hierarchies like Russell’s type theory forbid to avoid
paradox) and what that means for consistency and meaning. We also
reflect on the notion of <strong>nonsense</strong> itself: echoing
Wittgenstein’s insight that there are things that can only be shown but
not said, with outright nonsense sometimes pointing to the limits of
language (<a
href="https://plato.stanford.edu/entries/wittgenstein/#:~:text=%E2%80%98Nonsense%E2%80%99%20became%20the%20hinge%20of,The%20quandary%20arises%20concerning%20the">Ludwig
Wittgenstein (Stanford Encyclopedia of Philosophy)</a>) (<a
href="https://plato.stanford.edu/entries/wittgenstein/#:~:text=nonsensical,what%20can%20only%20be%20shown">Ludwig
Wittgenstein (Stanford Encyclopedia of Philosophy)</a>). In our theory,
nonsense ideas exist formally and we can <em>show</em> them (write their
structure) even if we cannot <em>say</em> what they mean – perhaps a
formal analog of Wittgenstein’s ladder (to be thrown away after
climbing) (<a
href="https://plato.stanford.edu/entries/wittgenstein/#:~:text=nonsensical,what%20can%20only%20be%20shown">Ludwig
Wittgenstein (Stanford Encyclopedia of Philosophy)</a>).</p>
<p>Finally, we explore <strong>Applications</strong> of this formal
theory. The ideas include: using the theory as a foundation for
<strong>knowledge representation</strong> in AI (with a uniform data
structure for all concepts, and the guarantee of unique identifiers for
concepts via the complexity measure); employing it in <strong>formal
ontology</strong> and library science (to avoid ambiguous concept
entries by ensuring structural uniqueness); connections to <strong>type
theory and programming language design</strong> (perhaps inspiring more
flexible type systems that can handle and reason about currently
“nonsensical” programs, improving languages’ ability to manipulate code
as data); <strong>cognitive science and psychology</strong> (modeling
how children might build complex concepts from primitive ones, or how
creative thought might combine concepts in novel ways – including
nonsensical combinations that could later become meaningful metaphors);
and <strong>mathematics/computer science</strong> (the theory can be
seen as an initial algebra or term algebra, which has applications in
theorem proving and symbolic computation – e.g. unique term
representation is crucial in automated reasoning (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=Freely%20generated%20algebraic%20structure%20over,a%20given%20signature">Term
algebra - Wikipedia</a>)). We will also mention potential use in
analyzing <strong>paradoxes and self-referential systems</strong>: by
representing them formally, we might separate the structural facet from
the semantic, possibly shedding light on what makes certain
self-referential ideas paradoxical. Throughout the applications, we
maintain an “academically skeptical” tone – noting that while the
formalism is intriguing and offers some clear benefits (like canonical
forms), one must be cautious about how much “real world” insight it
directly yields, given that we have intentionally left semantics
aside.</p>
<p>Before diving in, let us clarify our <strong>notation and
terminology</strong>. We will use the symbol <span
class="math inline"><em>I</em><sub>0</sub></span> to denote the
<em>primitive idea</em>, the sole atomic element we start with. (One
might also call it <span class="math inline"><em>P</em></span> for
“Primitive”, but we choose <span
class="math inline"><em>I</em><sub>0</sub></span> to signify “Idea at
level 0”.) In each version of the theory, new ideas are generated by
certain operations applied to existing ideas. We will use <span
class="math inline"><em>δ</em></span> as a generic notation for an
abstraction operation (its arity and interpretation will vary by
version). For example, in the unary setting, <span
class="math inline"><em>δ</em>(<em>X</em>)</span> will denote the
abstraction of idea <span class="math inline"><em>X</em></span> (a unary
operation). In the binary setting, <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> will denote
the abstraction of <span class="math inline"><em>Y</em></span> under
<span class="math inline"><em>X</em></span> (a binary operation). We
stick with the symbol <span class="math inline"><em>δ</em></span>
throughout for consistency, as a nod to the notion of a “difference” or
“derivative” (since each abstraction can be seen as a step difference
from the prior). The use of <span class="math inline"><em>δ</em></span>
in phrases like “δ-based complexity” will refer to the fact that our
complexity measure is derived from counting or encoding these <span
class="math inline"><em>δ</em></span> operations.</p>
<p>We emphasize that <strong>all results we prove are relative to the
formal system itself</strong>. For instance, when we prove uniqueness of
abstraction chains, we mean in the formal sense (no two distinct formal
derivations produce the same final idea structure). This is a
mathematical combinatorial fact. We are not claiming anything directly
about how humans think or how meaning works in natural language – though
in the philosophical discussion we will cautiously draw analogies. The
tone of our development will be formal and cautious: each claim will be
stated as a definition, axiom, or theorem, and we will either prove it
or explain it carefully along with any limitations. We remain aware that
a formal theory of ideas, no matter how internally consistent,
ultimately needs to interface with reality (cognitive or semantic) to be
fully meaningful – but that is beyond our present scope. By deliberately
isolating the structural aspect, we hope to provide a solid skeleton
that future work can flesh out with semantics and empirical
validation.</p>
<p>The structure of the paper follows the versions outlined: each of the
next four sections (Sections 3–6) corresponds to Versions 1–4 of the
theory, each building on the previous. Section 7 then delves into
<strong>Philosophical Meaning</strong>, Section 8 discusses
<strong>Applications</strong>, and Section 9 concludes.</p>
<h2 id="version-1-primitive-and-unary-abstraction">Version 1 – Primitive
and Unary Abstraction</h2>
<p>We begin with the most basic version of the theory. <strong>Version
1</strong> introduces the minimum ingredients: a single primitive idea
and one abstraction operation that creates a new idea from an existing
one. This corresponds to a simple <em>unary</em> generative process,
akin to a successor function in arithmetic or an “idea of an idea”
concept in naive terms. Despite its simplicity, we will formalize it
carefully and establish the important property of <em>unique abstraction
chains</em> in this setting, setting the stage for more complex
versions.</p>
<h3 id="definitions-and-axioms-version-1">Definitions and Axioms
(Version 1)</h3>
<p><strong>Definition 1.1 (Ideas in Version 1):</strong> There exists a
unique <em>primitive idea</em>, denoted <span
class="math inline"><em>I</em><sub>0</sub></span>. The set of all
<strong>ideas</strong> in Version 1, denoted <span
class="math inline">ℐ<sub>1</sub></span>, is defined inductively as
follows:</p>
<ul>
<li>(Base) <span
class="math inline"><em>I</em><sub>0</sub> ∈ ℐ<sub>1</sub></span>.</li>
<li>(Abstraction) If <span
class="math inline"><em>X</em> ∈ ℐ<sub>1</sub></span>, then <span
class="math inline"><em>δ</em>(<em>X</em>) ∈ ℐ<sub>1</sub></span>.</li>
<li>No other elements are in <span
class="math inline">ℐ<sub>1</sub></span> except those obtained by a
finite number of applications of the <span
class="math inline"><em>δ</em></span> operation starting from <span
class="math inline"><em>I</em><sub>0</sub></span>.</li>
</ul>
<p>In plain language, <span
class="math inline">ℐ<sub>1</sub> = {<em>I</em><sub>0</sub>, <em>δ</em>(<em>I</em><sub>0</sub>), <em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>)), <em>δ</em>(<em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>))), …}</span>
– an infinite sequence. We will sometimes denote <span
class="math inline"><em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
for the idea obtained by <span class="math inline"><em>n</em></span>
successive <span class="math inline"><em>δ</em></span> applications on
<span class="math inline"><em>I</em><sub>0</sub></span> (with <span
class="math inline"><em>δ</em><sup>0</sup>(<em>I</em><sub>0</sub>) := <em>I</em><sub>0</sub></span>
for convenience).</p>
<p><strong>Axiom 1.1 (Primitive Uniqueness):</strong> The primitive idea
<span class="math inline"><em>I</em><sub>0</sub></span> has no internal
structure and is unique. This means there is exactly one 0-ary idea (no
multiple distinct primitives). This axiom is analogous to the uniqueness
of the empty set in ZF set theory (extensionality implies there is only
one empty set (<a
href="https://iep.utm.edu/set-theo/#:~:text=match%20at%20L482%20The%20empty,denote%20the%20empty%20set">Set
Theory | Internet Encyclopedia of Philosophy</a>)).</p>
<p><strong>Axiom 1.2 (Structural Identity – Unary):</strong> Ideas are
identical if and only if they have the same construction history from
<span class="math inline"><em>I</em><sub>0</sub></span>. More formally,
we assume <span class="math inline"><em>δ</em></span> is an
<em>injective</em> operation on ideas. For any <span
class="math inline"><em>X</em>, <em>Y</em> ∈ ℐ<sub>1</sub></span>: if
<span
class="math inline"><em>δ</em>(<em>X</em>) = <em>δ</em>(<em>Y</em>)</span>,
then <span class="math inline"><em>X</em> = <em>Y</em></span>. Together
with the understanding that <span
class="math inline"><em>I</em><sub>0</sub></span> is unique, this means
each element of <span class="math inline">ℐ<sub>1</sub></span> has a
<em>unique predecessor</em> (except <span
class="math inline"><em>I</em><sub>0</sub></span> which has none). There
is no other identification or equivalence imposed on ideas. In
particular, <span
class="math inline"><em>δ</em>(<em>X</em>) ≠ <em>X</em></span> for any
<span class="math inline"><em>X</em></span> (no idempotence), and we do
not identify distinct <span
class="math inline"><em>δ</em></span>-iterations with each other.</p>
<p><em>Axiom 1.2 is fundamental:</em> it captures the notion that we
consider only the <em>structure</em> (here, the number of <span
class="math inline"><em>δ</em></span> steps from the primitive) as
determining identity. This is akin to a principle of extensionality in
this context – the “element” of an idea is its predecessor in the
abstraction chain. If two ideas have the same predecessor and were
produced by the same abstraction operation, and the operation is
injective, they must coincide. (This is trivially satisfied in this
unary setting, but we state it explicitly as it will generalize.)
Without this axiom, one could imagine a degenerate interpretation where
<span class="math inline"><em>δ</em>(<em>X</em>)</span> might sometimes
equal <span class="math inline"><em>δ</em>(<em>Y</em>)</span> even if
<span class="math inline"><em>X</em> ≠ <em>Y</em></span>, which would
ruin uniqueness of representation. We disallow that by fiat.</p>
<p>We do not need many more axioms at this stage – essentially we are
describing a free unary algebra on one generator. It is a <strong>term
algebra</strong> with a single constant symbol <span
class="math inline"><em>I</em><sub>0</sub></span> and a single unary
function symbol <span class="math inline"><em>δ</em></span>. By standard
results in universal algebra, this structure is an <strong>initial
algebra</strong> in its category and all terms are unique up to identity
(<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=Freely%20generated%20algebraic%20structure%20over,a%20given%20signature">Term
algebra - Wikipedia</a>). We will prove the uniqueness of abstraction
chains directly to be self-contained.</p>
<h3 id="theorems-and-properties-version-1">Theorems and Properties
(Version 1)</h3>
<p><strong>Theorem 1.1 (Existence and Uniqueness of Abstraction Chains
in <span class="math inline">ℐ<sub>1</sub></span>):</strong> Every idea
<span class="math inline"><em>X</em> ∈ ℐ<sub>1</sub></span> is of the
form <span
class="math inline"><em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
for a unique natural number <span
class="math inline"><em>n</em> ≥ 0</span>. In other words:</p>
<ol type="1">
<li>(Existence) For each <span
class="math inline"><em>X</em> ∈ ℐ<sub>1</sub></span>, there exists some
<span class="math inline"><em>n</em> ∈ ℕ</span> such that <span
class="math inline"><em>X</em> = <em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>.</li>
<li>(Uniqueness) If <span
class="math inline"><em>δ</em><sup><em>m</em></sup>(<em>I</em><sub>0</sub>) = <em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>,
then <span class="math inline"><em>m</em> = <em>n</em></span>.
Equivalently, each idea has a <em>unique</em> abstraction chain leading
back to <span class="math inline"><em>I</em><sub>0</sub></span>.</li>
</ol>
<p><em>Proof:</em> Existence is by the inductive definition of <span
class="math inline">ℐ<sub>1</sub></span>: by definition, any <span
class="math inline"><em>X</em> ∈ ℐ<sub>1</sub></span> is obtained by a
finite number of <span class="math inline"><em>δ</em></span>
applications on <span class="math inline"><em>I</em><sub>0</sub></span>.
So certainly <span
class="math inline"><em>X</em> = <em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
for <em>some</em> <span class="math inline"><em>n</em></span> (just take
<span class="math inline"><em>n</em></span> equal to that number of
applications). Uniqueness is proven by induction on the length of the
chain.</p>
<ul>
<li><strong>Base (<span
class="math inline"><em>n</em> = 0</span>):</strong> The only idea
obtained by zero <span class="math inline"><em>δ</em></span>
applications is <span class="math inline"><em>I</em><sub>0</sub></span>
itself. So there is no ambiguity for <span
class="math inline"><em>n</em> = 0</span>.</li>
<li><strong>Inductive Step:</strong> Suppose for some <span
class="math inline"><em>k</em> &lt; <em>N</em></span>, it is already
established that if <span
class="math inline"><em>δ</em><sup><em>m</em></sup>(<em>I</em><sub>0</sub>) = <em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
with <span
class="math inline"><em>m</em>, <em>n</em> ≤ <em>k</em></span>, then
<span class="math inline"><em>m</em> = <em>n</em></span>. Now consider
two representations of some idea as <span
class="math inline"><em>δ</em><sup><em>N</em></sup>(<em>I</em><sub>0</sub>)</span>
and <span
class="math inline"><em>δ</em><sup><em>M</em></sup>(<em>I</em><sub>0</sub>)</span>
with <span class="math inline"><em>M</em>, <em>N</em> &gt; 0</span>. We
have <span
class="math inline"><em>δ</em><sup><em>N</em></sup>(<em>I</em><sub>0</sub>) = <em>δ</em>(<em>δ</em><sup><em>N</em> − 1</sup>(<em>I</em><sub>0</sub>))</span>
and <span
class="math inline"><em>δ</em><sup><em>M</em></sup>(<em>I</em><sub>0</sub>) = <em>δ</em>(<em>δ</em><sup><em>M</em> − 1</sup>(<em>I</em><sub>0</sub>))</span>.
If these are equal, i.e. <span
class="math inline"><em>δ</em>(<em>δ</em><sup><em>N</em> − 1</sup>(<em>I</em><sub>0</sub>)) = <em>δ</em>(<em>δ</em><sup><em>M</em> − 1</sup>(<em>I</em><sub>0</sub>))</span>,
then by Axiom 1.2 (injectivity of <span
class="math inline"><em>δ</em></span>) we must have <span
class="math inline"><em>δ</em><sup><em>N</em> − 1</sup>(<em>I</em><sub>0</sub>) = <em>δ</em><sup><em>M</em> − 1</sup>(<em>I</em><sub>0</sub>)</span>.
Now <span class="math inline"><em>N</em> − 1</span> and <span
class="math inline"><em>M</em> − 1</span> are at most <span
class="math inline"><em>N</em> − 1 &lt; <em>N</em></span> and <span
class="math inline"><em>M</em> − 1 &lt; <em>M</em></span>, so at most
<span class="math inline"><em>N</em> − 1</span> (assuming WLOG <span
class="math inline"><em>N</em> ≥ <em>M</em></span>). By the inductive
hypothesis, this implies <span
class="math inline"><em>N</em> − 1 = <em>M</em> − 1</span>. Thus <span
class="math inline"><em>N</em> = <em>M</em></span>. This completes the
induction, proving uniqueness for all <span
class="math inline"><em>n</em></span>. □</li>
</ul>
<p>This theorem shows that the ideas in <span
class="math inline">ℐ<sub>1</sub></span> are in one-to-one
correspondence with the natural numbers (including 0). In fact, we can
define a simple complexity measure here: <span
class="math inline"><em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>x</em><em>i</em><em>t</em><em>y</em>(<em>I</em><sub>0</sub>) := 0</span>,
and <span
class="math inline"><em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>x</em><em>i</em><em>t</em><em>y</em>(<em>δ</em>(<em>X</em>)) := <em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>x</em><em>i</em><em>t</em><em>y</em>(<em>X</em>) + 1</span>.
By Theorem 1.1, <span
class="math inline"><em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>x</em><em>i</em><em>t</em><em>y</em>(<em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)) = <em>n</em></span>,
and if <span
class="math inline"><em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>x</em><em>i</em><em>t</em><em>y</em>(<em>X</em>) = <em>c</em><em>o</em><em>m</em><em>p</em><em>l</em><em>e</em><em>x</em><em>i</em><em>t</em><em>y</em>(<em>Y</em>)</span>
then <span class="math inline"><em>X</em> = <em>Y</em></span> in this
version (since that would imply they are <span
class="math inline"><em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
and <span
class="math inline"><em>δ</em><sup><em>m</em></sup>(<em>I</em><sub>0</sub>)</span>
with the same <span class="math inline"><em>n</em> = <em>m</em></span>).
So the complexity measure (which in this simple case is just the chain
length) <em>uniquely determines the structure</em> of the idea. It’s a
very trivial kind of δ-based complexity: essentially the count of
δ-steps. We note this formally:</p>
<p><strong>Corollary 1.2 (δ-Complexity in Version 1):</strong> Define
<span
class="math inline"><em>δ</em>-<em>l</em><em>e</em><em>n</em>(<em>X</em>)</span>
for <span class="math inline"><em>X</em> ∈ ℐ<sub>1</sub></span> as the
unique <span class="math inline"><em>n</em></span> such that <span
class="math inline"><em>X</em> = <em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>.
Then <span
class="math inline"><em>δ</em>-<em>l</em><em>e</em><em>n</em> : ℐ<sub>1</sub> → ℕ</span>
is a bijection. In particular, it is injective, so the value <span
class="math inline"><em>δ</em>-<em>l</em><em>e</em><em>n</em>(<em>X</em>)</span>
uniquely identifies <span class="math inline"><em>X</em></span>. Thus
the “δ-based complexity” (here just the length of the abstraction chain)
uniquely determines the idea’s structure.</p>
<p><em>Proof:</em> This is immediate from Theorem 1.1. <span
class="math inline"><em>δ</em>-<em>l</em><em>e</em><em>n</em></span> is
well-defined by existence and uniqueness of <span
class="math inline"><em>n</em></span>. It is injective by uniqueness. It
is also clearly surjective onto <span class="math inline">ℕ</span>
because for every <span class="math inline"><em>n</em></span> we have
<span
class="math inline"><em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
in the codomain. □</p>
<p>In more narrative terms, in Version 1 the theory of ideas is
essentially equivalent to the theory of natural number counting. We have
a genesis story: <span class="math inline"><em>I</em><sub>0</sub></span>
is the first idea (think of it as “the primordial notion”) and <span
class="math inline"><em>δ</em></span> generates “the idea of that idea”
(which is <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>)</span>), and then
<span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>))</span>
= “the idea of the idea of the idea”, and so on. Each idea is nothing
but a stack of “idea-of” operators applied to the primitive. There is an
obvious structural order here: we can say <span
class="math inline"><em>δ</em><sup><em>m</em></sup>(<em>I</em><sub>0</sub>)</span>
is more <em>complex</em> than <span
class="math inline"><em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
if <span class="math inline"><em>m</em> &gt; <em>n</em></span>, since it
has more abstraction steps. And since the chain is unique, there’s no
ambiguity in such comparisons.</p>
<p><strong>Limitation of Version 1:</strong> This system, while clear,
is not very powerful in expressive terms. All it can represent is a
single infinite hierarchy of abstracted concepts, each one an “abstract
generalization” of the previous in some unspecified way. If one tries to
assign an intuitive meaning, perhaps <span
class="math inline"><em>I</em><sub>0</sub></span> could be some raw
sense-datum, <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>)</span> an
abstract concept from that, <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>))</span>
an abstract concept from the previous abstract concept, etc. But without
additional structure, these abstractions don’t branch or diversify;
there’s essentially only one line of thought. In formal terms, <span
class="math inline">ℐ<sub>1</sub></span> is in bijection with <span
class="math inline">ℕ</span>, so it lacks the richness to encode varied
content or multiple dimensions.</p>
<p>Another limitation: there is no notion of combining two separate
ideas into a new one. Each new idea comes from exactly one parent idea.
If we thought of ideas as sets (just as an analogy), this is like a
universe where every set has exactly one element (its predecessor),
forming a simple chain <span class="math inline">{{{⋯{⌀}⋯}}}</span>.
Such a universe is a very degenerate case of set theory. Indeed, in set
theory one typically can combine two sets <span
class="math inline"><em>A</em></span> and <span
class="math inline"><em>B</em></span> (e.g. form <span
class="math inline">{<em>A</em>, <em>B</em>}</span>); here we have
nothing analogous.</p>
<p>However, <strong>Version 1 establishes the pattern of
reasoning</strong> we will use: inductive construction and proving
uniqueness by induction and injectivity of constructors. The simplicity
of this version also highlights the role of <em>structural
identity</em>. If we were to consider a scenario where <span
class="math inline"><em>δ</em>(<em>X</em>) = <em>δ</em>(<em>Y</em>)</span>
but <span class="math inline"><em>X</em> ≠ <em>Y</em></span>, that would
violate Axiom 1.2; we have forbidden that, as it would mean two
different chains collapse into one idea. In Version 1, that kind of
collapse can’t happen anyway unless the operation <span
class="math inline"><em>δ</em></span> itself had some algebraic property
(like idempotence <span
class="math inline"><em>δ</em>(<em>X</em>) = <em>X</em></span> or
periodicity <span
class="math inline"><em>δ</em><sup><em>k</em></sup>(<em>X</em>) = <em>X</em></span>
for some <span class="math inline"><em>k</em></span>) – but we assume it
doesn’t.</p>
<p>Finally, we note that <strong>semantic equivalence is completely
trivial here</strong> – since we have no semantics, we can’t even say
something like “<span
class="math inline"><em>δ</em><sup>2</sup>(<em>I</em><sub>0</sub>)</span>
has the same meaning as <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>)</span>”, because
we have not assigned any meaning at all. All differences are structural.
This means even if one had an urge to say “<span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>))</span>
might mean the same as <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>)</span> in some
context”, within the theory they are distinct (because one is two steps
from <span class="math inline"><em>I</em><sub>0</sub></span> and the
other is one step). We enforce this strictly. In a sense, Version 1 is a
formal system that doesn’t <em>need</em> semantics to be fully
understood; it’s purely syntactical and combinatorial.</p>
<p>Having built this simplest model, we move on to <strong>Version
2</strong>, which enriches the abstraction process by introducing
multiple modes of abstraction, addressing the first limitation (lack of
branching diversity).</p>
<h2 id="version-2-multi-modal-unary-abstraction">Version 2 – Multi-Modal
Unary Abstraction</h2>
<p>Version 2 extends the unary abstraction framework by allowing
<strong>multiple abstraction operations (modes)</strong> instead of just
one. This means that from any given idea, there may be more than one way
to abstract a new idea from it, each way labeled or distinguished by a
mode. For example, one might have a mode <span
class="math inline"><em>A</em></span> and a mode <span
class="math inline"><em>B</em></span> such that from idea <span
class="math inline"><em>X</em></span> one can get <span
class="math inline"><em>δ</em><sub><em>A</em></sub>(<em>X</em>)</span>
or <span
class="math inline"><em>δ</em><sub><em>B</em></sub>(<em>X</em>)</span>,
which are considered different ideas. This adds a branching factor to
the idea-generation process: the structure of an idea is no longer just
how many abstraction steps, but also <em>which sequence of modes</em>
were chosen at each step.</p>
<p>This version is motivated by the observation that in human thought or
in classification systems, we often have different “kinds” of
abstraction or different aspects under which one can abstract from a
concept. For instance, given a concrete concept like “a red apple,” one
might abstract to “fruit” (abstracting color away) or to “red thing”
(abstracting shape/type away), etc. Those are different modes of
abstraction yielding different results. In our formal setting, we won’t
attach meanings like color or shape – we simply allow the formal
possibility of different abstraction modalities.</p>
<h3 id="definitions-and-axioms-version-2">Definitions and Axioms
(Version 2)</h3>
<p><strong>Definition 2.1 (Modes and Ideas in Version 2):</strong> Let
<span class="math inline"><em>M</em></span> be a non-empty index set of
<em>modes</em>. (This could be finite or infinite; for generality we can
take <span class="math inline"><em>M</em></span> countably infinite or
even uncountable, but it won’t matter – even two modes would illustrate
the concept.) For each mode <span
class="math inline"><em>m</em> ∈ <em>M</em></span>, we have a unary
abstraction operation <span
class="math inline"><em>δ</em><sub><em>m</em></sub>(⋅)</span>. The set
of all ideas in Version 2, <span
class="math inline">ℐ<sub>2</sub></span>, is defined inductively by:</p>
<ul>
<li><span
class="math inline"><em>I</em><sub>0</sub> ∈ ℐ<sub>2</sub></span> (the
same unique primitive as before).</li>
<li>If <span class="math inline"><em>X</em> ∈ ℐ<sub>2</sub></span> and
<span class="math inline"><em>m</em> ∈ <em>M</em></span>, then <span
class="math inline"><em>δ</em><sub><em>m</em></sub>(<em>X</em>) ∈ ℐ<sub>2</sub></span>.</li>
<li>No other elements are in <span
class="math inline">ℐ<sub>2</sub></span>.</li>
</ul>
<p>Thus, every idea in <span class="math inline">ℐ<sub>2</sub></span>
can be seen as a finite <em>word</em> of mode symbols from <span
class="math inline"><em>M</em></span> applied to <span
class="math inline"><em>I</em><sub>0</sub></span>. For example, <span
class="math inline"><em>δ</em><sub><em>a</em></sub>(<em>I</em><sub>0</sub>)</span>
(using a generic <span
class="math inline"><em>a</em> ∈ <em>M</em></span>) is a first-level
abstract idea of type <span class="math inline"><em>a</em></span>, <span
class="math inline"><em>δ</em><sub><em>b</em></sub>(<em>δ</em><sub><em>a</em></sub>(<em>I</em><sub>0</sub>))</span>
would be an idea obtained by first abstracting in mode <span
class="math inline"><em>a</em></span>, then from that result abstracting
in mode <span class="math inline"><em>b</em></span>, denoted <span
class="math inline"><em>δ</em><sub><em>b</em></sub>(<em>δ</em><sub><em>a</em></sub>(<em>I</em><sub>0</sub>))</span>
(or succinctly <span
class="math inline"><em>δ</em><sub><em>b</em></sub><em>δ</em><sub><em>a</em></sub>(<em>I</em><sub>0</sub>)</span>).
In general, an element of <span class="math inline">ℐ<sub>2</sub></span>
is of the form <span
class="math inline"><em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub><em>δ</em><sub><em>m</em><sub><em>n</em> − 1</sub></sub>⋯<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>)</span>
for some finite sequence <span
class="math inline"><em>m</em><sub>1</sub>, <em>m</em><sub>2</sub>, ..., <em>m</em><sub><em>n</em></sub></span>
of modes from <span class="math inline"><em>M</em></span>. We will also
write this as <span
class="math inline"><em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub>(<em>δ</em><sub><em>m</em><sub><em>n</em> − 1</sub></sub>(⋯<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>)⋯))</span>,
or abbreviate it by a string of mode labels <span
class="math inline"><em>m</em><sub>1</sub><em>m</em><sub>2</sub>⋯<em>m</em><sub><em>n</em></sub></span>
applied to <span class="math inline"><em>I</em><sub>0</sub></span>. For
instance, if <span
class="math inline"><em>M</em> = {<em>A</em>, <em>B</em>}</span>, one
idea could be <span
class="math inline"><em>A</em><em>B</em><em>A</em>(<em>I</em><sub>0</sub>)</span>
as a shorthand for <span
class="math inline"><em>δ</em><sub><em>A</em></sub>(<em>δ</em><sub><em>B</em></sub>(<em>δ</em><sub><em>A</em></sub>(<em>I</em><sub>0</sub>)))</span>.</p>
<p><strong>Axiom 2.1 (Structural Identity – Multi-Modal):</strong> The
structural identity criterion now entails that <strong>each mode of
abstraction is injective and distinct</strong>. Formally, for any modes
<span class="math inline"><em>m</em>, <em>m</em>′ ∈ <em>M</em></span>
and any ideas <span
class="math inline"><em>X</em>, <em>Y</em> ∈ ℐ<sub>2</sub></span>: if
<span
class="math inline"><em>δ</em><sub><em>m</em></sub>(<em>X</em>) = <em>δ</em><sub><em>m</em>′</sub>(<em>Y</em>)</span>,
then <em>both</em> <span
class="math inline"><em>m</em> = <em>m</em>′</span> and <span
class="math inline"><em>X</em> = <em>Y</em></span>. In other words, two
results of abstraction are equal only if they were obtained by the same
mode from equal source ideas. This implies in particular that for a
given mode <span class="math inline"><em>m</em></span>, <span
class="math inline"><em>δ</em><sub><em>m</em></sub></span> is injective
(no two different <span
class="math inline"><em>X</em>, <em>Y</em></span> yield the same <span
class="math inline"><em>δ</em><sub><em>m</em></sub>(<em>X</em>) = <em>δ</em><sub><em>m</em></sub>(<em>Y</em>)</span>),
and that different modes never coincide on outputs (if <span
class="math inline"><em>m</em> ≠ <em>m</em>′</span>, <span
class="math inline"><em>δ</em><sub><em>m</em></sub>(<em>X</em>)</span>
can never equal <span
class="math inline"><em>δ</em><sub><em>m</em>′</sub>(<em>Y</em>)</span>,
no matter what <span class="math inline"><em>X</em>, <em>Y</em></span>
are).</p>
<p>Axiom 2.1 generalizes Axiom 1.2. We still have one primitive <span
class="math inline"><em>I</em><sub>0</sub></span> that is unique. Now we
have multiple <span class="math inline"><em>δ</em></span> operations,
but we require that the structure (the exact sequence of operations) is
recoverable from the final product. This is essentially saying the
algebra of ideas is a <strong>free term algebra over the
signature</strong> consisting of one constant <span
class="math inline"><em>I</em><sub>0</sub></span> and unary function
symbols <span
class="math inline">{<em>δ</em><sub><em>m</em></sub> : <em>m</em> ∈ <em>M</em>}</span>.
Such a term algebra is known to have unique readability: each term
(idea) can be uniquely parsed into its construction steps (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=Freely%20generated%20algebraic%20structure%20over,a%20given%20signature">Term
algebra - Wikipedia</a>). Axiom 2.1 is the no-identification condition
that ensures we indeed have the free algebra, not some quotient where
different sequences yield the same result.</p>
<p>We carry over <strong>Axiom 1.1 (Primitive Uniqueness)</strong>
unchanged: there is still exactly one primitive idea <span
class="math inline"><em>I</em><sub>0</sub></span>.</p>
<p>Intuitively, the axioms mean that the <strong>path matters</strong>.
For example, <span
class="math inline"><em>δ</em><sub><em>A</em></sub>(<em>I</em><sub>0</sub>)</span>
and <span
class="math inline"><em>δ</em><sub><em>B</em></sub>(<em>I</em><sub>0</sub>)</span>
are distinct by virtue of <span
class="math inline"><em>A</em> ≠ <em>B</em></span>. Also, if <span
class="math inline"><em>δ</em><sub><em>A</em></sub>(<em>I</em><sub>0</sub>) = <em>δ</em><sub><em>A</em></sub>(<em>δ</em><sub><em>B</em></sub>(<em>I</em><sub>0</sub>))</span>,
then by injectivity we would deduce <span
class="math inline"><em>I</em><sub>0</sub> = <em>δ</em><sub><em>B</em></sub>(<em>I</em><sub>0</sub>)</span>,
which cannot be (since <span
class="math inline"><em>I</em><sub>0</sub></span> is primitive and <span
class="math inline"><em>δ</em><sub><em>B</em></sub>(<em>I</em><sub>0</sub>)</span>
is a “child” of it, not equal unless some identification is forced). We
have no such identifications, so indeed all these objects remain
distinct.</p>
<h3 id="theorems-and-properties-version-2">Theorems and Properties
(Version 2)</h3>
<p><strong>Theorem 2.1 (Unique Abstraction Chains in <span
class="math inline">ℐ<sub>2</sub></span>):</strong> Every idea <span
class="math inline"><em>X</em> ∈ ℐ<sub>2</sub></span> can be written
uniquely as <span
class="math inline"><em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub><em>δ</em><sub><em>m</em><sub><em>n</em> − 1</sub></sub>⋯<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>)</span>
for some finite sequence of modes <span
class="math inline"><em>m</em><sub>1</sub>, ..., <em>m</em><sub><em>n</em></sub> ∈ <em>M</em></span>.
In particular, the sequence (or “chain”) of modes leading from <span
class="math inline"><em>I</em><sub>0</sub></span> to <span
class="math inline"><em>X</em></span> is unique.</p>
<p><em>Proof (Sketch):</em> This is a straightforward generalization of
the proof of Theorem 1.1, now using induction on the length of the mode
sequence. We simultaneously argue existence and uniqueness:</p>
<ul>
<li><p><em>Existence:</em> By definition, any <span
class="math inline"><em>X</em> ∈ ℐ<sub>2</sub></span> is obtained by
some finite number of mode applications on <span
class="math inline"><em>I</em><sub>0</sub></span>. So <span
class="math inline"><em>X</em> = <em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub>(...<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>)...)</span>
for some sequence <span
class="math inline"><em>m</em><sub>1</sub>, ..., <em>m</em><sub><em>n</em></sub></span>.
That’s by construction.</p></li>
<li><p><em>Uniqueness:</em> Suppose <span
class="math inline"><em>X</em></span> can be represented by two mode
sequences: <span
class="math inline"><em>X</em> = <em>δ</em><sub><em>m</em><sub><em>k</em></sub></sub>⋯<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>) = <em>δ</em><sub><em>m</em>′<sub><em>l</em></sub></sub>⋯<em>δ</em><sub><em>m</em>′<sub>1</sub></sub>(<em>I</em><sub>0</sub>)</span>.
We need to show these sequences are identical term-by-term. We do
induction on the length of the sequences. If one sequence has length 0,
then <span
class="math inline"><em>X</em> = <em>I</em><sub>0</sub></span>; the
other must also yield <span
class="math inline"><em>I</em><sub>0</sub></span>. The only sequence
that yields <span class="math inline"><em>I</em><sub>0</sub></span> is
the empty sequence (since applying any <span
class="math inline"><em>δ</em></span> produces a non-primitive idea). So
if one representation has <span
class="math inline"><em>n</em> = 0</span>, the other must too; trivial
case. Now assume both representations have length <span
class="math inline"> ≥ 1</span>. Then <span
class="math inline"><em>X</em> = <em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub>(<em>Y</em>)</span>
for <span
class="math inline"><em>Y</em> = <em>δ</em><sub><em>m</em><sub><em>n</em> − 1</sub></sub>⋯<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>)</span>,
and also <span
class="math inline"><em>X</em> = <em>δ</em><sub><em>m</em>′<sub><em>l</em></sub></sub>(<em>Y</em>′)</span>
for some <span class="math inline"><em>Y</em>′</span> from the other
sequence. Now <span
class="math inline"><em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub>(<em>Y</em>) = <em>δ</em><sub><em>m</em>′<sub><em>l</em></sub></sub>(<em>Y</em>′)</span>.
By Axiom 2.1, we deduce that <span
class="math inline"><em>m</em><sub><em>n</em></sub> = <em>m</em>′<sub><em>l</em></sub></span>
(the last modes must match) and <span
class="math inline"><em>Y</em> = <em>Y</em>′</span>. So in fact both
sequences end in the same mode and produce the same predecessor idea
<span class="math inline"><em>Y</em> = <em>Y</em>′</span>. Now we can
cancel that last step and conclude <span
class="math inline"><em>Y</em></span> has two representations by shorter
sequences (of length <span class="math inline"><em>n</em> − 1</span> and
<span class="math inline"><em>l</em> − 1</span> respectively). By the
inductive hypothesis on smaller length, those two sequences for <span
class="math inline"><em>Y</em></span> must be identical. Appending the
same final mode <span
class="math inline"><em>m</em><sub><em>n</em></sub> = <em>m</em>′<sub><em>l</em></sub></span>
to them, we conclude the full sequences for <span
class="math inline"><em>X</em></span> are identical. □</p></li>
</ul>
<p>This theorem confirms that <span
class="math inline">ℐ<sub>2</sub></span> has a one-to-one correspondence
with sequences (words) over the alphabet <span
class="math inline"><em>M</em></span>. If we think of <span
class="math inline"><em>M</em></span> as an alphabet, each idea is like
a unique “string” of mode symbols, reading from the primitive outward.
In computer science terms, <span
class="math inline">ℐ<sub>2</sub></span> is the set of all ground terms
over a signature of unary function symbols <span
class="math inline"><em>M</em></span> and constant <span
class="math inline"><em>I</em><sub>0</sub></span>. Such terms indeed
have a unique parse (chain) by construction.</p>
<p>Given this correspondence, we can define a <strong>complexity
measure</strong> in many ways. One natural measure is again the length
of the sequence (the number of abstraction steps). Another could
incorporate the alphabet symbols to ensure uniqueness (though length
alone won’t uniquely identify the idea, it’s a part of the story). A
more precise complexity measure could encode the entire sequence as a
number. For instance, we could fix an encoding of each mode <span
class="math inline"><em>m</em> ∈ <em>M</em></span> as an integer or bit
string, and then encode a sequence <span
class="math inline"><em>m</em><sub>1</sub>...<em>m</em><sub><em>n</em></sub></span>
as a single integer via a combinatorial encoding (like interleaving bits
or using a prime factorization trick). One straightforward method for an
injective encoding is to use a pairing function iteratively: for
example, assign each mode <span class="math inline"><em>m</em></span> a
unique positive integer code <span
class="math inline">#(<em>m</em>)</span>. Then we can define a
Gödel-style encoding <span
class="math inline"><em>C</em><em>o</em><em>d</em><em>e</em>(<em>I</em><sub>0</sub>) = 1</span>
(or 0) and <span
class="math inline"><em>C</em><em>o</em><em>d</em><em>e</em>(<em>δ</em><sub><em>m</em></sub>(<em>X</em>)) = 2<sup>#(<em>m</em>)</sup> ⋅ <em>C</em><em>o</em><em>d</em><em>e</em>(<em>X</em>)</span>.
If we choose all mode codes <span
class="math inline">#(<em>m</em>)</span> distinct, this multiplication
by a distinct power of 2 each time yields a unique number (leveraging
the uniqueness of prime factorizations: the exponents of 2 in the final
number will record the multiset of modes used). A simpler approach:
treat the pair <span
class="math inline">(#(<em>m</em>),<em>n</em>)</span> as one step and
encode by Cantor pairing or something. But we need not get bogged down:
the <strong>existence</strong> of some injective complexity measure is
guaranteed by the fact that the set of finite sequences over <span
class="math inline"><em>M</em></span> is countable (if <span
class="math inline"><em>M</em></span> is countable) or at least
well-orderable if <span class="math inline"><em>M</em></span> is
well-orderable, etc. We can always assign a unique ordinal or natural
number to each sequence.</p>
<p><strong>Corollary 2.2 (Complexity Measure in Version 2):</strong>
There exists a function <span
class="math inline"><em>κ</em> : ℐ<sub>2</sub> → ℕ</span> (assuming
<span class="math inline"><em>M</em></span> is countable; or to ordinals
if uncountable) such that <span class="math inline"><em>κ</em></span> is
injective. One simple choice is <span
class="math inline"><em>κ</em>(<em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub>⋯<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>))=</span>
the natural number whose base-<span
class="math inline">|<em>M</em>|</span> representation is the
concatenation of the code digits for <span
class="math inline"><em>m</em><sub>1</sub>, ..., <em>m</em><sub><em>n</em></sub></span>.
In any case, one can assign each idea a unique <strong>δ-code</strong>
or <strong>δ-complexity index</strong>. Thus, <span
class="math inline"><em>κ</em>(<em>X</em>) = <em>κ</em>(<em>Y</em>)</span>
if and only if <span class="math inline"><em>X</em> = <em>Y</em></span>
in <span class="math inline">ℐ<sub>2</sub></span>. In particular, if we
just take the length and the sequence itself, that pair uniquely
determines the structure.</p>
<p><em>Proof Sketch:</em> Using Theorem 2.1, <span
class="math inline"><em>X</em></span> and <span
class="math inline"><em>Y</em></span> have unique mode sequences. If
those sequences are different, one can always find a way to map
sequences to numbers injectively (for example, fix a bijection <span
class="math inline"><em>f</em> : <em>M</em> → {1, ..., |<em>M</em>|}</span>
if <span class="math inline"><em>M</em></span> is finite, or to <span
class="math inline">ℕ</span> if <span
class="math inline"><em>M</em></span> is countable; then define <span
class="math inline"><em>κ</em>(<em>I</em><sub>0</sub>) = 0</span>, and
for a sequence <span
class="math inline"><em>m</em><sub>1</sub>...<em>m</em><sub><em>n</em></sub></span>
define <span
class="math inline"><em>κ</em>(<em>δ</em><sub><em>m</em><sub><em>n</em></sub></sub>...<em>δ</em><sub><em>m</em><sub>1</sub></sub>(<em>I</em><sub>0</sub>))</span>
as <span class="math inline">$\sum_{i=1}^n f(m_i) \cdot
(|M|)^{i-1}$</span> if <span class="math inline"><em>M</em></span> is
finite – this is basically treating the mode sequence as a base-<span
class="math inline">|<em>M</em>|</span> number with digits <span
class="math inline"><em>f</em>(<em>m</em><sub><em>i</em></sub>)</span>).
This is clearly injective: different sequences give different sums
because they differ in some digit place. If <span
class="math inline"><em>M</em></span> is infinite countable, one can do
a similar trick by encoding mode labels with distinct prime powers or
using Cantor pairing repeatedly. The key point is that since the
representation of each idea is unique, encoding that representation
yields a unique code. So <span class="math inline"><em>κ</em></span>
exists and is injective. □</p>
<p>This corollary formalizes the idea that <strong>δ-based complexity
uniquely determines structure</strong> in Version 2. However, note that
unlike Version 1, here “complexity” cannot be a single natural number
like the length alone – length alone is not injective (many different
ideas have the same number of steps). We need the whole sequence or an
encoding of it. Usually by “complexity” one might expect a numeric
measure like length or something akin to Kolmogorov complexity. In our
context, <span class="math inline"><em>κ</em>(<em>X</em>)</span> is more
like a <em>description</em> (the actual code of the construction) rather
than a simple scalar measure. One could of course consider the pair
(length, lexicographic rank) as the complexity.</p>
<p>If we analogize to <strong>Kolmogorov complexity</strong>, there is a
similarity: Kolmogorov complexity of a string is the length of the
shortest program generating it (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>). Here, because each idea
<em>is</em> generated by one particular “program” (the sequence of
modes) which is in fact the shortest and only program to generate it,
the length of that program and the program itself serve as a description
of the idea. In a sense, every idea in <span
class="math inline">ℐ<sub>2</sub></span> has a <em>unique minimal
description</em> in terms of the primitive and abstraction steps,
reminiscent of an idea that “the shortest description of an object is
unique if we fix a descriptive language” (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>). Our descriptive language is
the mode sequence, and given our axioms, no object has two different
descriptions, so the shortest description is also the only description.
(This is a bit idealized compared to true Kolmogorov complexity where
multiple programs could produce the same output string but we look for
the shortest; here, multiple programs producing the same output is
disallowed by design.)</p>
<p><strong>Discussion:</strong> Version 2 yields a <strong>tree</strong>
of ideas branching from <span
class="math inline"><em>I</em><sub>0</sub></span>. If <span
class="math inline"><em>M</em></span> has <span
class="math inline"><em>k</em></span> elements, the branching factor is
up to <span class="math inline"><em>k</em></span> at each node (like a
<span class="math inline"><em>k</em></span>-ary tree, though <span
class="math inline"><em>k</em></span> unary operations is effectively
the same as <span class="math inline"><em>k</em></span>-ary branching in
the tree of generation). If <span class="math inline"><em>M</em></span>
is infinite, it’s like a tree with possibly infinitely many children at
each node. Regardless, it’s a hierarchy reminiscent of a <strong>type
hierarchy or ontology</strong> (though extremely generic). We can think
of <span class="math inline"><em>I</em><sub>0</sub></span> as the most
concrete or base concept, and each mode <span
class="math inline"><em>m</em></span> as an aspect in which we can
generalize or abstract. For instance, if we were to give a toy
semantics: imagine <span
class="math inline"><em>I</em><sub>0</sub></span> = “a specific object”.
<span
class="math inline"><em>δ</em><sub>Color</sub>(<em>I</em><sub>0</sub>)</span>
might mean “the color of that object” (abstracting away everything but
color), while <span
class="math inline"><em>δ</em><sub>Shape</sub>(<em>I</em><sub>0</sub>)</span>
means “the shape of that object”. Then <span
class="math inline"><em>δ</em><sub>Generic</sub>(<em>δ</em><sub>Color</sub>(<em>I</em><sub>0</sub>))</span>
might further abstract color to just “being colored” or something. This
is a purely illustrative semantic overlay – formally we do not ascribe
such interpretations, but it’s useful to see how modes could correspond
to conceptual operations like projection or generalization of certain
attributes.</p>
<p>In this formal version, still <strong>no two different ideas
coincide</strong>, so nonsense is still not particularly evident because
with multiple modes we don’t yet have an obvious notion of a
contradictory or meaningless combination – except one could say an idea
like <span
class="math inline"><em>δ</em><sub>Color</sub>(<em>δ</em><sub>Shape</sub>(<em>I</em><sub>0</sub>))</span>
might be “the color of the shape of object” which, if you try to
interpret, could be nonsensical (color of a shape?). But formally it’s
perfectly fine: it’s just <span
class="math inline"><em>δ</em><sub>Color</sub>(<em>δ</em><sub>Shape</sub>(<em>I</em><sub>0</sub>))</span>,
a distinct node in the tree. Our system <em>allows it</em> even if
interpretatively one might scratch their head. This highlights the
point: <strong>conceptual nonsense is formally allowed</strong>. If the
modes were intended to have some real-world meaning, not every
composition of them may make sense in reality; however, we do not
prevent the formal combination. This can be justified in a similar vein
as in mathematics we allow algebraic expressions that might not make
semantic sense under certain interpretations until evaluated or
restricted (like you can write <span
class="math inline">$\sqrt{-1}$</span> even if you’re initially working
over reals, it just means something outside the reals until you extend
the number system). Our formalism doesn’t complain about <span
class="math inline"><em>δ</em><sub>Color</sub>(<em>δ</em><sub>Shape</sub>(<em>I</em><sub>0</sub>))</span>;
it is a valid idea. It would be up to a semantic theory later to say
maybe that idea has no interpretation (or find one, e.g., maybe the
shape has a color if we imagine shape embodied in an object – one can
contrive something, but anyway).</p>
<p><strong>Limitation of Version 2:</strong> The major limitation
remaining is that ideas cannot be combined <em>horizontally</em>. We
only have the vertical tree-growing via unary abstraction. Suppose we
have two different ideas <span class="math inline"><em>X</em></span> and
<span class="math inline"><em>Y</em></span> that both descended from
<span class="math inline"><em>I</em><sub>0</sub></span> (so they share
the ultimate ancestor but are distinct branches). There is no operation
in Version 2 that takes <span class="math inline"><em>X</em></span> and
<span class="math inline"><em>Y</em></span> together to form a new idea.
Each new idea always has exactly one parent. In terms of expressiveness,
this means we cannot capture something like a <em>combination</em> of
two concepts or a relation between two ideas as a new idea. For example,
if one mode gives “fruit” and another branch gives “red”, we cannot
directly form an idea corresponding to “red fruit” by combining the idea
“fruit” and the idea “red”. In Version 2, “red fruit” would have to
appear as a result of some unary abstraction from either “fruit” or
“red” or <span class="math inline"><em>I</em><sub>0</sub></span>, which
doesn’t reflect combining independent concepts – it would enshrine one
as prior and then abstract the other from it. But conceptually “red
fruit” is more like combining the property “red” with the category
“fruit”.</p>
<p>This signals that to talk about combining separate idea structures,
we need a binary (or at least multi-ary) operation. That leads us to
<strong>Version 3</strong>, where we introduce a binary abstraction
allowing one idea to function as a modifier or operator on another.</p>
<p>Before proceeding, let’s highlight: <strong>Set theory, type theory,
and category theory parallels (for Version 2).</strong> In set theory,
by this stage (analogy) we have something like a von Neumann hierarchy
if each <span class="math inline"><em>δ</em><sub><em>m</em></sub></span>
were like some operation of adding a set marker. But not exactly, set
theory would allow union of two sets, which we can’t do yet. In type
theory, Version 2 is reminiscent of a simple type system where <span
class="math inline"><em>I</em><sub>0</sub></span> is a ground type and
we have unary type constructors (like list of X, option of X, etc.) –
each mode is like a type constructor giving a new type from an old type.
Our theorem of unique representation is akin to the principle that types
have unique construction trees (which holds in well-formed type
expressions without equivalences). Indeed, <span
class="math inline"><em>δ</em><sub><em>m</em></sub></span> being
distinct corresponds to type constructors being distinct, and
injectivity corresponds to no different subtypes collapse – a property
known in some formalizations as no “junk” or no “confusion” in algebraic
specifications (<a
href="https://www.cs.ubc.ca/sites/default/files/tr/1983/TR-83-02.pdf#:~:text=,of%20the%20intended%20set">[PDF]
DATA TYPES AS TERM ALGEBRAS - UBC Computer Science</a>). In category
theory, one might see <span class="math inline">ℐ<sub>2</sub></span> as
the free term algebra, which is the <em>initial object</em> in the
category of all <span class="math inline"><em>M</em></span>-unary
algebras. The unique chain property corresponds to initiality: any
morphism out of the initial algebra is defined by its action on the
generator and extends uniquely (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=signature%2C%20and%20this%20object%2C%20unique,5">Term
algebra - Wikipedia</a>). While we won’t deep dive into category theory
here, it’s worth noting the structural similarity: we have essentially
defined an <strong>initial term model</strong> for a given signature, a
concept well-known in categorical algebra and universal algebra.</p>
<p>Now, we turn to the next advancement: introducing a binary
abstraction operation to allow idea combinations.</p>
<h2 id="version-3-functional-binary-abstraction">Version 3 – Functional
Binary Abstraction</h2>
<p>Version 3 introduces the possibility of combining two ideas to
produce a new idea. We call it <strong>functional binary
abstraction</strong> because we conceptualize one idea as playing the
role of a function (or operator) and the other as its argument or
operand. This is akin to application in lambda calculus or function
application in general: if <span class="math inline"><em>F</em></span>
and <span class="math inline"><em>X</em></span> are ideas, we allow an
idea that represents “<span class="math inline"><em>F</em></span>
abstracted over <span class="math inline"><em>X</em></span>”. We will
denote this by <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>, using the
same letter <span class="math inline"><em>δ</em></span> now as a binary
operation.</p>
<p>It is important to clarify that in this version we are not simply
adding a binary operation <em>alongside</em> the unary ones of Version
2; rather, we are <strong>supplanting or generalizing</strong> the
abstraction concept to binary form. In practice, we can incorporate both
unary and binary (the unary being a special case when one argument is a
fixed distinguished idea or something). But to keep the presentation
streamlined, we will treat the binary abstraction as the primary
operation. One can simulate the earlier unary modes by particular
choices of one of the arguments. For example, a mode <span
class="math inline"><em>m</em></span> from Version 2 can be represented
by an idea <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> such that
applying <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> to an input
<span class="math inline"><em>X</em></span> via <span
class="math inline"><em>δ</em>(<em>F</em><sub><em>m</em></sub>,<em>X</em>)</span>
yields the effect of <span
class="math inline"><em>δ</em><sub><em>m</em></sub>(<em>X</em>)</span>.
That is, in a sense, the unary operator <span
class="math inline"><em>δ</em><sub><em>m</em></sub></span> is now
<em>encoded as</em> a certain idea <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> which is used
in a binary application. This approach is analogous to how in
combinatory logic or lambda calculus, you might have a constant function
(like a combinator) that when applied yields a specific transformation
of the argument.</p>
<p><strong>Goals for Version 3:</strong> - Allow any idea to be used as
a function on any other idea. - Maintain uniqueness of representation. -
Define complexity measure accordingly. - See the explosion of possible
structures (graphs that are not simple tree chains but trees with binary
nodes). - Recognize that now truly “nonsensical” combinations can occur
freely (since any <span class="math inline"><em>F</em></span> can apply
to any <span class="math inline"><em>X</em></span>, even if <span
class="math inline"><em>F</em></span> wasn’t intended to be a function
of an <span class="math inline"><em>X</em></span>). - Connect this with
concept of <em>self-application</em> (when <span
class="math inline"><em>F</em> = <em>X</em></span> in <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>, i.e. an
idea applied to itself, a form of self-reference).</p>
<h3 id="definitions-and-axioms-version-3">Definitions and Axioms
(Version 3)</h3>
<p><strong>Definition 3.1 (Ideas in Version 3):</strong> The set of
ideas <span class="math inline">ℐ<sub>3</sub></span> is generated as
follows:</p>
<ul>
<li><span
class="math inline"><em>I</em><sub>0</sub> ∈ ℐ<sub>3</sub></span> (the
same unique primitive).</li>
<li>If <span class="math inline"><em>F</em> ∈ ℐ<sub>3</sub></span> and
<span class="math inline"><em>X</em> ∈ ℐ<sub>3</sub></span>, then <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>) ∈ ℐ<sub>3</sub></span>.</li>
<li>No other elements are in <span
class="math inline">ℐ<sub>3</sub></span>.</li>
</ul>
<p>Here <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> is a
<em>binary</em> operation. We will often refer to <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> as
“applying <span class="math inline"><em>F</em></span> to <span
class="math inline"><em>X</em></span>” or “the abstraction of <span
class="math inline"><em>X</em></span> by <span
class="math inline"><em>F</em></span>”. We might sometimes use notation
like <span class="math inline"><em>F</em> ∘ <em>X</em></span> or <span
class="math inline"><em>F</em>(<em>X</em>)</span> for brevity in
discussions, but formally we keep <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>.</p>
<p>This definition means that every idea in <span
class="math inline">ℐ<sub>3</sub></span> is either the primitive <span
class="math inline"><em>I</em><sub>0</sub></span> or a binary tree whose
nodes are labeled with <span class="math inline"><em>δ</em></span> and
whose leaves are <span
class="math inline"><em>I</em><sub>0</sub></span>. More concretely, each
non-primitive idea has a form <span
class="math inline"><em>δ</em>(<em>A</em>,<em>B</em>)</span> where <span
class="math inline"><em>A</em>, <em>B</em> ∈ ℐ<sub>3</sub></span> are
“smaller” ideas (closer to <span
class="math inline"><em>I</em><sub>0</sub></span> presumably). This is
the hallmark of an <strong>algebra of terms with one binary
operation</strong>. It is the term algebra for a signature with one
2-ary function symbol <span class="math inline"><em>δ</em></span> and
one constant <span
class="math inline"><em>I</em><sub>0</sub></span>.</p>
<p><strong>Axiom 3.1 (Structural Identity – Binary):</strong> The pair
<span class="math inline">(<em>F</em>,<em>X</em>)</span> is recoverable
from <span class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>.
Formally, <span class="math inline"><em>δ</em></span> is an injective
function of two arguments. If <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>) = <em>δ</em>(<em>F</em>′,<em>X</em>′)</span>,
then <span class="math inline"><em>F</em> = <em>F</em>′</span> and <span
class="math inline"><em>X</em> = <em>X</em>′</span>. This ensures no two
distinct pairs of ideas yield the same combined idea. Equivalently,
<span class="math inline"><em>δ</em></span> as a constructor has the
property of unique decomposition: an idea of the form <span
class="math inline"><em>δ</em>(<em>Y</em>,<em>Z</em>)</span> cannot
equal an idea of the form <span
class="math inline"><em>δ</em>(<em>Y</em>′,<em>Z</em>′)</span> unless
<span class="math inline"><em>Y</em> = <em>Y</em>′</span> and <span
class="math inline"><em>Z</em> = <em>Z</em>′</span>.</p>
<p>Axiom 3.1 is again a no-identification (or “no confusion” (<a
href="https://www.cs.ubc.ca/sites/default/files/tr/1983/TR-83-02.pdf#:~:text=,of%20the%20intended%20set">[PDF]
DATA TYPES AS TERM ALGEBRAS - UBC Computer Science</a>)) axiom,
analogous to extensionality in set pairs (which implies if <span
class="math inline">{<em>a</em>, <em>b</em>} = {<em>c</em>, <em>d</em>}</span>
then presumably <span
class="math inline"><em>a</em> = <em>c</em>, <em>b</em> = <em>d</em></span>
up to order, though here order matters). We do not impose any
commutativity or symmetry on <span
class="math inline"><em>δ</em></span>; <span
class="math inline">(<em>F</em>,<em>X</em>)</span> is an <em>ordered
pair</em>. So <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>) ≠ <em>δ</em>(<em>X</em>,<em>F</em>)</span>
in general unless <span
class="math inline"><em>F</em> = <em>X</em></span> (we do not identify
those structures).</p>
<p>We also carry over the extensional uniqueness of the primitive <span
class="math inline"><em>I</em><sub>0</sub></span>. So implicitly: -
<span class="math inline"><em>I</em><sub>0</sub></span> is distinct from
any <span class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>
(since <span class="math inline"><em>δ</em></span> always yields
something “built” not primitive). - There is only one primitive (so you
can’t have two distinct things that behave as primitives; trivial
here).</p>
<p><strong>Remark on encoding Version 2 in Version 3:</strong> If we
want to represent a unary mode <span
class="math inline"><em>m</em></span> from before, we can introduce an
idea <span
class="math inline"><em>F</em><sub><em>m</em></sub> = <em>δ</em>(<em>I</em><sub>0</sub>,<em>X</em><sub><em>m</em></sub>)</span>
for some <em>particular <span
class="math inline"><em>X</em><sub><em>m</em></sub></span></em>.
Actually, since <span class="math inline"><em>I</em><sub>0</sub></span>
is the only constant, <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> itself must
ultimately be built from <span
class="math inline"><em>I</em><sub>0</sub></span>. Perhaps a simpler
scheme: designate that an application of the form <span
class="math inline"><em>δ</em>(<em>F</em><sub><em>m</em></sub>,<em>X</em>)</span>
corresponds to the unary abstraction <span
class="math inline"><em>δ</em><sub><em>m</em></sub>(<em>X</em>)</span>.
To enforce that <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> behaves
distinctly for each <span class="math inline"><em>m</em></span>, <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> could be a
structure that “carries” the identity of <span
class="math inline"><em>m</em></span> in it. For example, if we had
retained modes, we could artificially enforce structure. But
systematically, one could fix some scheme like: define a “mode idea”
<span class="math inline"><em>M</em></span> as <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
(some canonical structure), then define <span
class="math inline"><em>F</em><sub><em>m</em></sub></span> as <span
class="math inline"><em>δ</em>(<em>M</em><sup><em>m</em></sup>,<em>I</em><sub>0</sub>)</span>
i.e. apply <span class="math inline"><em>M</em></span> to itself <span
class="math inline"><em>m</em></span> times in some encoding (if <span
class="math inline"><em>m</em></span> is like a number or index). The
details aren’t too important; the takeaway is that Version 3 is at least
as expressive as Version 2 in principle because we can encode any unary
abstraction as using a particular functional idea applied to the input.
Version 3 actually vastly exceeds Version 2 in expressive power because
now <em>any</em> idea can serve as an abstraction operator, not just
some designated ones.</p>
<h3 id="theorems-and-properties-version-3">Theorems and Properties
(Version 3)</h3>
<p><strong>Theorem 3.1 (Unique Construction Tree in <span
class="math inline">ℐ<sub>3</sub></span>):</strong> Every idea <span
class="math inline"><em>X</em> ∈ ℐ<sub>3</sub></span> can be uniquely
represented as a binary tree with leaves <span
class="math inline"><em>I</em><sub>0</sub></span>. Formally, if we
regard <span class="math inline">ℐ<sub>3</sub></span> as the set of
ground terms over the signature <span
class="math inline">{<em>δ</em>(⋅,⋅), <em>I</em><sub>0</sub>}</span>,
then each <span class="math inline"><em>X</em></span> is such a term. If
<span class="math inline"><em>X</em></span> can be written as <span
class="math inline"><em>δ</em>(<em>A</em>,<em>B</em>)</span> and also as
<span class="math inline"><em>δ</em>(<em>A</em>′,<em>B</em>′)</span>
(possibly in multiple ways by different parenthesizations), the result
of Axiom 3.1 is that the top-level decomposition is unique: <span
class="math inline"><em>A</em> = <em>A</em>′</span> and <span
class="math inline"><em>B</em> = <em>B</em>′</span>. By applying this
property recursively down the tree, it follows that the entire tree
representation of <span class="math inline"><em>X</em></span> is unique.
Equivalently, the set <span class="math inline">ℐ<sub>3</sub></span> is
the <em>free algebra</em> generated by <span
class="math inline"><em>I</em><sub>0</sub></span> with one binary
operation, so the usual unique parse/unique decomposition property
holds.</p>
<p><em>Proof Sketch:</em> The formal proof is by induction on the size
of the term (like term rewriting or structural induction arguments). A
more direct reasoning: <span class="math inline">ℐ<sub>3</sub></span> as
defined is <em>exactly</em> the term algebra (or term model) of the
algebraic specification with one constant and one binary operator. It is
well-known that term algebras have unique readability: no term can be
read (parsed) in two different ways. Specifically, if <span
class="math inline"><em>X</em></span> is not <span
class="math inline"><em>I</em><sub>0</sub></span>, then by the
definition it <em>must</em> be of the form <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>. There is
no other way to form a non-primitive idea. So the <em>only</em> possible
top-level parse of <span class="math inline"><em>X</em></span> is as
<span class="math inline"><em>δ</em>(⋅,⋅)</span>. Now if <span
class="math inline"><em>X</em> = <em>δ</em>(<em>P</em>,<em>Q</em>)</span>
for some <span class="math inline"><em>P</em>, <em>Q</em></span>, that
representation is unique by Axiom 3.1: there is no other pair <span
class="math inline"><em>P</em>′, <em>Q</em>′</span> with <span
class="math inline"><em>δ</em>(<em>P</em>′,<em>Q</em>′) = <em>X</em></span>.
So the root separation is unique. Then apply the same argument to <span
class="math inline"><em>P</em></span> and <span
class="math inline"><em>Q</em></span> themselves: by induction, their
sub-structure is uniquely determined. Ultimately you bottom out at <span
class="math inline"><em>I</em><sub>0</sub></span> leaves which are
obviously uniform. Thus, each <span
class="math inline"><em>X</em></span> corresponds to exactly one
binary-tree structure. □</p>
<p>This theorem is analogous to Theorem 2.1 but now for binary trees
instead of sequences. It assures us that we haven’t inadvertently
introduced some algebraic identification (like <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>
equaling <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>))</span>
or something weird — no, they are distinct since one is <span
class="math inline"><em>δ</em>(<em>A</em>,<em>I</em><sub>0</sub>)</span>
and the other is <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>B</em>)</span>
with <span
class="math inline"><em>A</em> = <em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
and <span
class="math inline"><em>B</em> = <em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>,
and Axiom 3.1 says those aren’t equal unless <span
class="math inline"><em>A</em> = <em>I</em><sub>0</sub></span> and <span
class="math inline"><em>B</em> = <em>I</em><sub>0</sub></span>, which
they are not in that example).</p>
<p>So <span class="math inline">ℐ<sub>3</sub></span> is isomorphic to
the set of all finite full binary trees (each internal node with two
children) whose leaf nodes are all labeled with <span
class="math inline"><em>I</em><sub>0</sub></span>. Another way to see
it: any <span class="math inline"><em>X</em> ∈ ℐ<sub>3</sub></span> can
be written as <span
class="math inline"><em>δ</em><sup><em>n</em></sup>(<em>I</em><sub>0</sub>)</span>
but in a two-dimensional notation – that notation doesn’t linearize well
because the order of δ applications can differ (non-associative). But we
can linearize by some bracketed notation: e.g., <span
class="math inline"><em>X</em> = (((<em>I</em><sub>0</sub>*<em>I</em><sub>0</sub>)*<em>I</em><sub>0</sub>)*(<em>I</em><sub>0</sub>*<em>I</em><sub>0</sub>))</span>
as a representation (here using * for <span
class="math inline"><em>δ</em></span>) – that fully parenthesized
expression is unique for <span class="math inline"><em>X</em></span>.
This is essentially the parse tree.</p>
<p>Given this uniqueness, we can once again assert the existence of a
<strong>δ-based complexity measure</strong>. However, now the measure
needs to encode a binary tree structure, not just a linear string or
length. There are known methods: one could do a Cantor pairing or Gödel
encoding of a pair. For example, we can define <span
class="math inline"><em>C</em><em>o</em><em>d</em><em>e</em>(<em>I</em><sub>0</sub>) = 0</span>.
For a general idea <span
class="math inline"><em>X</em> = <em>δ</em>(<em>F</em>,<em>X</em>)</span>,
one can define <span
class="math inline"><em>C</em><em>o</em><em>d</em><em>e</em>(<em>X</em>) = 2<sup><em>C</em><em>o</em><em>d</em><em>e</em>(<em>F</em>)</sup> ⋅ 3<sup><em>C</em><em>o</em><em>d</em><em>e</em>(<em>X</em>)</sup></span>
(use two distinct primes 2 and 3 to encode the pair) or any injective
pairing function <span
class="math inline"><em>N</em> × <em>N</em> → <em>N</em></span>. This
will be injective because the prime power factorization is unique – the
exponents (which are Code(F) and Code(X)) can be recovered uniquely (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=signature%2C%20and%20this%20object%2C%20unique,5">Term
algebra - Wikipedia</a>). Another scheme: treat the tree as a bitstring
using a traversal encoding (e.g. encode it in parentheses: maybe
“(()())” style, then map that to a binary sequence 1/0, then to an
integer). The details are not particularly important to derive here, but
we can assert:</p>
<p><strong>Corollary 3.2 (Injective Complexity Coding in Version
3):</strong> There exists an injective function <span
class="math inline"><em>κ</em> : ℐ<sub>3</sub> → ℕ</span> that assigns
each idea a unique code. For instance, define <span
class="math display"><em>κ</em>(<em>I</em><sub>0</sub>) = 1,   <em>κ</em>(<em>δ</em>(<em>F</em>,<em>X</em>)) = pair(<em>κ</em>(<em>F</em>), <em>κ</em>(<em>X</em>)),</span>
where <span class="math inline">pair(<em>a</em>,<em>b</em>)</span> is a
fixed pairing function (like <span
class="math inline">pair(<em>a</em>,<em>b</em>) = 2<sup><em>a</em></sup> ⋅ (2<em>b</em>+1)</span>
or Cantor’s pairing <span
class="math inline">(1/2)(<em>a</em>+<em>b</em>)(<em>a</em>+<em>b</em>+1) + <em>b</em></span>).
By the property of pairing functions, <span
class="math inline"><em>κ</em></span> is one-to-one (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=,T%7D%7D%28X%29%7D%20to%20its%20corresponding">Term
algebra - Wikipedia</a>). Thus, a δ-based complexity measure (the code
<span class="math inline"><em>κ</em>(<em>X</em>)</span>) uniquely
determines <span class="math inline"><em>X</em></span>’s entire
structure.</p>
<p>The proof is basically constructing <span
class="math inline"><em>κ</em></span> using the uniqueness of tree
representation. Since each idea’s structure is unique, we just encode
that structure. We cited one possible pairing approach: using distinct
prime bases (2 and 3) is particularly straightforward and leverages
unique prime factorization (<a
href="https://plato.stanford.edu/entries/nonwellfounded-set-theory/#:~:text=The%20term%20non,Foundation%20Axiom">Non-wellfounded
Set Theory (Stanford Encyclopedia of Philosophy)</a>). For example,
<span
class="math inline"><em>κ</em>(<em>δ</em>(<em>F</em>,<em>X</em>)) = 2<sup><em>κ</em>(<em>F</em>)</sup>3<sup><em>κ</em>(<em>X</em>)</sup></span>.
No two distinct pairs <span
class="math inline">(<em>F</em>,<em>X</em>)</span> will yield the same
<span
class="math inline">2<sup><em>a</em></sup>3<sup><em>b</em></sup></span>
because if <span
class="math inline">2<sup><em>a</em></sup>3<sup><em>b</em></sup> = 2<sup><em>a</em>′</sup>3<sup><em>b</em>′</sup></span>,
equating prime exponents gives <span
class="math inline"><em>a</em> = <em>a</em>′</span> and <span
class="math inline"><em>b</em> = <em>b</em>′</span>. Thus it’s
injective.</p>
<p>So again, <em>complexity uniquely determines structure</em>. However,
now the “complexity” can no longer be summarized as a single simple
number like “the number of abstraction steps” – because even the number
of steps (number of internal nodes) is not enough. For example, <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>))</span>
and <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>
both have 2 abstraction nodes (so length 2), but they are different
structures. Our complexity measure might give those two distinct codes
(e.g. one might code to <span
class="math inline">2<sup>1</sup>3<sup>2<sup>1</sup>3<sup>1</sup></sup></span>
vs the other <span
class="math inline">2<sup>2<sup>1</sup>3<sup>1</sup></sup>3<sup>1</sup></span>,
which are different numbers indeed).</p>
<p>It might be that by “δ-based complexity” the intention was some
measure of how “complex” an idea is in terms of generative steps.
Perhaps the number of nodes or height could be considered complexity
metrics too: - The <strong>depth</strong> (max chain length from root to
leaf) of the idea’s tree might measure one kind of complexity (like how
abstract it got in one branch). - The <strong>size</strong> (total
number of <span class="math inline"><em>I</em><sub>0</sub></span>
occurrences or <span class="math inline"><em>δ</em></span> nodes) is
another measure (like how large the structure is). - The theorem
actually gives a stronger statement: not only length or depth, but the
entire structure is encoded.</p>
<p>However, the phrasing “<em>δ-based complexity uniquely determines
structure</em>” suggests maybe that the complexity measure we use is
inherently tied to δ (the operations count). It’s safe to interpret that
as we did: we count or encode δ operations in such a way that the code
is unique.</p>
<p>Now, with Version 3, we have a qualitatively new phenomenon:
<strong>functional abstraction and conceptual nonsense</strong>. Since
any idea can appear in the function position, we can have situations
like: - <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> where <span
class="math inline"><em>X</em></span> was not “meant” to be a function
of <span class="math inline"><em>Y</em></span> in some semantic sense.
For example, let <span
class="math inline"><em>X</em> = <em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
(this is an idea that might stand for something like “some composite
concept”) and <span
class="math inline"><em>Y</em> = <em>I</em><sub>0</sub></span> (the
primitive). <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>) = <em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>.
If we foolishly try to interpret <span
class="math inline"><em>X</em></span> as (say) the concept of “apple”
and <span class="math inline"><em>Y</em></span> as “tomato”, then <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> might
correspond to something like “apple tomato”? which is nonsense. Or if
<span class="math inline"><em>X</em></span> was “red” and <span
class="math inline"><em>Y</em></span> was “fruit”, <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> could be
imagined as “red applied to fruit” = “red fruit” which <em>is</em> a
meaningful concept “red fruit.” So some applications will have intuitive
meaning (if one argument is something like an adjective concept and the
other a noun concept, one can read an interpretation in natural
language), but others will not (if both arguments are nouns, “apple
tomato” doesn’t directly mean anything, though maybe a tomato that is an
apple? It’s odd). The formal system does not differentiate – any <span
class="math inline"><em>X</em></span> can apply to any <span
class="math inline"><em>Y</em></span>.</p>
<p>This is akin to an <strong>untyped lambda calculus</strong> or
<strong>combinatory logic</strong> in spirit: those systems allow any
term to be applied to any other term (some combinations may lead to
irreducible terms that don’t reduce to meaningful normal forms, but the
system itself is fine with that). By contrast, in a strongly typed
system (like a typed lambda calculus or categorical combinators), you
would restrict which applications are allowed (ensuring, e.g., that
<span class="math inline"><em>X</em></span> is a function that expects
input of the type of <span class="math inline"><em>Y</em></span>). Our
approach is deliberately untyped, meaning we allow “type errors” as
legitimate terms. In programming language lingo, our system will happily
consider the term <em>1 + “hi”</em> as a valid term structure (if we had
a way to represent numbers and strings), whereas a type checker would
reject it as nonsense (<a
href="https://carloangiuli.com/courses/b619-sp24/notes.pdf#:~:text=type%02checker%20may%20reject%20as%20nonsense,scoped">Principles
of Dependent Type Theory</a>). Indeed, recall the type theory reference:
a typechecker rejects <code>1 + "hi"</code> as nonsense because no
successful evaluation is possible (<a
href="https://carloangiuli.com/courses/b619-sp24/notes.pdf#:~:text=type%02checker%20may%20reject%20as%20nonsense,scoped">Principles
of Dependent Type Theory</a>). Similarly, in our theory, there’s nothing
preventing an “addition” idea to apply to a “word” idea, producing a new
idea that might not correspond to any real concept. We <em>embrace</em>
that scenario in formal terms; we just call it another idea.</p>
<p>The justification for allowing this “conceptual nonsense” formally is
that we want the formal system to be <strong>maximally
inclusive</strong> so that it doesn’t bias or restrict what counts as an
idea. If later some ideas turn out semantically void or contradictory,
that would be addressed by interpretation or additional semantic
constraints, not by the core formal generation rules. This is similar to
how set theory will form sets that might have no finite or concrete
interpretation, or how category theory says there could be a morphism
between any objects even if you didn’t intend one (unless restricted by
a specific category). Another viewpoint is <strong>paraconsistent
logic</strong> or naive frameworks: sometimes it’s fruitful to include
contradictions/nonsense and then manage them, rather than exclude them
upfront.</p>
<p>Now, because Version 3 introduces the possibility of
<em>self-application</em> as well (take <span
class="math inline"><em>F</em> = <em>X</em></span> in <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>, i.e. <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span>), we can
formalize something akin to self-referential ideas. For example, let’s
call <span
class="math inline"><em>S</em> = <em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>.
Then consider <span
class="math inline"><em>δ</em>(<em>S</em>,<em>S</em>) = <em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>))</span>.
This <span
class="math inline"><em>Z</em> = <em>δ</em>(<em>S</em>,<em>S</em>)</span>
is an idea which is structurally a binary tree where both left and right
subtrees are identical (each is <span
class="math inline"><em>S</em></span>). According to our rules it’s
perfectly fine. If one tried to interpret <span
class="math inline"><em>S</em></span> as some concept <span
class="math inline"><em>C</em></span>, then <span
class="math inline"><em>Z</em></span> would be “<span
class="math inline"><em>C</em></span> applied to <span
class="math inline"><em>C</em></span>”. If <span
class="math inline"><em>C</em></span> means an operation, that’s a
self-application. If <span class="math inline"><em>C</em></span> is a
statement, it’s a self-referential statement in a way. The structure
allows it. There is no paradox in just having the structure; paradoxes
come if we assign it a truth value or a set membership meaning perhaps.
But formally, <span
class="math inline"><em>δ</em>(<em>S</em>,<em>S</em>)</span> poses no
problem. It’s a fixed point of sorts: <span
class="math inline"><em>Z</em></span> contains a copy of itself?
Actually, <span class="math inline"><em>Z</em></span> contains <span
class="math inline"><em>S</em></span> which contains <span
class="math inline"><em>I</em><sub>0</sub></span> twice, not exactly
itself. If we tried to solve <span
class="math inline"><em>Z</em> = <em>δ</em>(<em>Z</em>,<em>X</em>)</span>
for some <span class="math inline"><em>X</em></span>, that would be
self-containing in a different way (like <span
class="math inline"><em>Z</em></span> appears as a subterm of itself).
That might require infinite regress, which our definition disallowed by
requiring finite generation steps (no infinite descending chain allowed
in well-founded term algebra).</p>
<p>We might note: we did implicitly assume <em>foundation</em> (no
infinite descending chain of abstractions, or no infinitely nested <span
class="math inline"><em>δ</em></span> without reaching <span
class="math inline"><em>I</em><sub>0</sub></span>). In a formal sense,
<span class="math inline">ℐ<sub>3</sub></span> as we defined are finite
terms. We did not include the possibility of an infinitely deep tree
(which would be like a limit object). If we allowed that, we could
represent perhaps a self-referential infinite idea like a solution to
<span
class="math inline"><em>X</em> = <em>δ</em>(<em>X</em>,<em>X</em>)</span>.
But we didn’t; all ideas are finite by inductive definition. So
something like <span
class="math inline"><em>X</em> = <em>δ</em>(<em>X</em>,<em>X</em>)</span>
has no solution in <span class="math inline">ℐ<sub>3</sub></span>
(except trivial infinite one if we extended to some completed domain).
So, we <em>avoid actual paradoxes</em> by construction (foundation in
set terms), but we allow wide-ranging finite combos.</p>
<p><strong>Comparison to Set Theory:</strong> At this stage, there’s an
interesting parallel: in ZF set theory, to avoid paradoxes like
Russell’s, one employs the Foundation (Regularity) axiom that no set is
an element of itself or of an infinite regress (<a
href="https://plato.stanford.edu/entries/nonwellfounded-set-theory/#:~:text=The%20term%20non,Foundation%20Axiom">Non-wellfounded
Set Theory (Stanford Encyclopedia of Philosophy)</a>). Our construction
similarly avoids <span class="math inline"><em>δ</em></span>-cycles by
building only well-founded trees. We might reflect: if we removed that
well-foundedness and allowed solutions to recursive definitions, we’d
get something akin to non-well-founded sets (Aczel’s AFA) where <span
class="math inline"><em>X</em> = {<em>X</em>}</span> has a unique
solution in that theory (<a
href="https://plato.stanford.edu/entries/nonwellfounded-set-theory/#:~:text=The%20term%20non,Foundation%20Axiom">Non-wellfounded
Set Theory (Stanford Encyclopedia of Philosophy)</a>). Similarly, <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> could equal
<span class="math inline"><em>X</em></span> in a non-well-founded
extension. That would be truly self-referential (like a loop). We do not
go that far here (though one could conceive a Version 3* that allows
cyclical definitions, which would complicate uniqueness since a finite
graph with a cycle can’t be unfolded uniquely into a tree unless you
allow infinite unfolding).</p>
<p><strong>Limitation of Version 3:</strong> The system now can combine
any two ideas, which is very powerful, but still something is “missing”
from an interpretive standpoint: we have not attached <em>any semantics
or meaning to the structure beyond structure itself</em>. The formal
system blindly generates combinations. One could ask: can we distinguish
within the system those combinations that appear “meaningful” from those
that don’t? At this stage, <em>no</em>, because the system doesn’t know
about meaning. However, one might try to impose some <em>patterns</em>
as having special significance. For example, maybe an idea of the form
<span
class="math inline"><em>δ</em>(<em>F</em>,<em>I</em><sub>0</sub>)</span>
we interpret as something like applying <span
class="math inline"><em>F</em></span> to the base object (like a unary
mode represented by <span class="math inline"><em>F</em></span>). Or a
pattern like <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>X</em>)</span>
could be interpreted as something (like maybe <span
class="math inline"><em>I</em><sub>0</sub></span> as a function could
mean an identity operation or a marker meaning “such that”? Hard to
say).</p>
<p>This leads to the thinking: perhaps we want to <em>recover some
semantics externally</em> by looking at structural patterns. This
motivates Version 4, where we drastically minimize the system to a
single binary constructor and one primitive, and consider how semantics
might arise from recognizing recurring patterns in these binary trees.
We effectively already have that scenario in Version 3 (since we have
one constructor anyway; we just also had implicitly an infinite class of
objects to use as possible functions). In Version 4, we won’t add new
operations, we will just emphasize patterns and minimality, possibly
eliminating even the distinction of a separate primitive constant beyond
<span class="math inline"><em>I</em><sub>0</sub></span>.</p>
<p>One more observation on Version 3: In terms of known theories: - It
resembles an <strong>applicative term system</strong> (like combinator
trees). Actually, <span class="math inline">ℐ<sub>3</sub></span> is
isomorphic to the set of all S-expressions in Lisp with one
distinguished atomic symbol. In Lisp terms, if we only have one atom
<code>I0</code> and one binary operation (which you could consider as
making a cons cell or application), then every S-expression is a binary
tree with <code>I0</code> at leaves. This is basically the idea of a
<strong>universal data structure</strong> (like Lisp cons) that can
represent arbitrarily complex structures. We know Lisp can represent any
data with cons and atoms, and even any program. In fact, Church encoding
or combinator encoding can represent lambda calculus in such a term
algebra. We have essentially reached a similarly universal
representational capability: <em>any possible finite discrete structure
can be encoded as an idea in <span
class="math inline">ℐ<sub>3</sub></span>.</em> Perhaps not labeled
structure, but unlabeled binary tree structure with a single label at
leaves – but labeling can be encoded by structure as well given enough
depth.</p>
<ul>
<li>In category theory terms, we constructed the <em>initial
algebra</em> for the functor <span
class="math inline"><em>X</em> ↦ 1 + <em>X</em> × <em>X</em></span> (one
for the leaf and product for the binary node) which is the set of all
full binary trees. That initial algebra has combinatorial significance
(Catalan numbers count the number of trees of a given size etc.). It’s
like the simplest non-trivial algebraic data type beyond lists. It’s
known that such structures are universal for computation if you allow
encoding of symbols.</li>
</ul>
<p>So Version 3 is already extremely general. But to fully align with
the prompt, we proceed to <strong>Version 4</strong>, where we’ll impose
“minimality” – probably meaning we use just the structure from Version 3
but with no extra distinct element aside from <span
class="math inline"><em>I</em><sub>0</sub></span> and <span
class="math inline"><em>δ</em></span>, and discuss
<strong>pattern-driven semantics</strong>.</p>
<h2
id="version-4-minimal-binary-abstraction-with-pattern-driven-semantics">Version
4 – Minimal Binary Abstraction with Pattern-Driven Semantics</h2>
<p>Version 4 is the culmination of our development: the system is
reduced to its <strong>minimal</strong> form, and we address how one
might begin to attach or recognize <strong>semantics from structural
patterns</strong> in this system.</p>
<p><strong>Minimality</strong> here means we want the absolute smallest
number of primitives and operations necessary to still generate a rich
universe of ideas as in Version 3. Looking at Version 3, we already have
only one operation <span class="math inline"><em>δ</em></span>. The only
other primitive is the constant <span
class="math inline"><em>I</em><sub>0</sub></span>. We cannot remove
<span class="math inline"><em>I</em><sub>0</sub></span> because then
we’d have no starting point to generate ideas (the term algebra would be
empty or only trivial infinite terms). So <span
class="math inline"><em>I</em><sub>0</sub></span> must stay. Could we
reduce anything else? In Version 3, we did not have any other symbols,
except one could argue we allowed ourselves many <em>elements</em> (the
ideas themselves) which could serve as “function values”. But those are
generated, not primitive. So indeed, Version 3 was already minimal in
terms of signature: one constant, one binary operator.</p>
<p>So what distinguishes Version 4? Possibly an emphasis on interpreting
patterns, and maybe an explicit statement that <em>everything is just
<span class="math inline"><em>δ</em></span> and <span
class="math inline"><em>I</em><sub>0</sub></span> now</em>. In practice,
Version 4 might not add new formal rules but rather add a layer of
discussion about <strong>semantic interpretation</strong>:</p>
<ul>
<li><strong>Pattern-Driven Semantics:</strong> means that although the
formal system has no semantic axioms, we (as theorists or as part of an
interpreting system) can assign meaning to certain <em>patterns of
ideas</em>. A pattern could be a particular shape of the tree or a
particular arrangement of <span
class="math inline"><em>I</em><sub>0</sub></span> and <span
class="math inline"><em>δ</em></span>. For example, maybe <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>X</em>)</span>
might consistently be interpreted as some kind of identity or projection
pattern, while <span
class="math inline"><em>δ</em>(<em>X</em>,<em>I</em><sub>0</sub>)</span>
might be another kind (just hypothesizing).</li>
</ul>
<p>We might also consider that in Version 4, <em>modes</em> or
<em>tags</em> from Version 2 are now entirely encoded as patterns of
<span class="math inline"><em>I</em><sub>0</sub></span> and <span
class="math inline"><em>δ</em></span>. So if we want to simulate a mode
A or B, we need to define an idea structure that represents that mode as
a function. This could be done by selecting some pattern as a surrogate
for “mode A operator”.</p>
<p>One possible systematic approach: Represent each mode by a Church
numeral style: - Let <span
class="math inline"><em>I</em><sub>0</sub></span> represent maybe a
“base” concept. - Use <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
as a distinguished idea <span class="math inline"><em>M</em></span> (we
might call it “mark” or “distinction” concept). - Use <span
class="math inline"><em>δ</em>(<em>M</em>,<em>I</em><sub>0</sub>)</span>
maybe to represent mode 1’s function, <span
class="math inline"><em>δ</em>(<em>M</em>,<em>M</em>)</span> to
represent mode 2’s function, or such.</p>
<p>Alternatively: - Maybe <span
class="math inline"><em>I</em><sub>0</sub></span> itself can appear in
either left or right position in a pattern to encode a bit. For
instance, an idea of pattern <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
is a simplest non-primitive. <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>))</span>
and <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>
are two distinct structures of size 2. We could assign one of them to
mean “Mode A operator” and the other “Mode B operator”. Then applying
them to something means layering that pattern.</p>
<p>Let’s illustrate a small <strong>pattern semantics example</strong>:
- Interpret <span class="math inline"><em>I</em><sub>0</sub></span> as
representing a specific object or a specific concept (the base of all).
- Interpret <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
as the concept “Distinct” or maybe “Generalization-1” or simply label it
“A”. - Interpret <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>
as concept “B”. - Now if we want to say “apply mode A to X”, we form
<span class="math inline"><em>δ</em>(<em>A</em>,<em>X</em>)</span>,
which in structure is <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>X</em>)</span>
(since <span
class="math inline"><em>A</em> = <em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>).
- If we want “apply mode B to X”: <span
class="math inline"><em>B</em></span> we said is <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>,
so <span
class="math inline"><em>δ</em>(<em>B</em>,<em>X</em>) = <em>δ</em>(<em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>),<em>X</em>)</span>.
- These are patterns. We could say any idea of the form <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>X</em>)</span>
we interpret as “A of X”, and any of form <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>),<em>X</em>)</span>
as “B of X”.</p>
<p>This is just one possible scheme. The point is: <strong>patterns can
carry meaning because we, the interpreter, assign them meaning</strong>.
The formal system itself treats <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>X</em>)</span>
and <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>),<em>X</em>)</span>
as just two different structures without inherent labels “A” or “B”. But
we, observing the pattern, can label the first pattern as meaning
something (like mode A applied to X) and the second as something
else.</p>
<p>One might ask: how many patterns do we have to play with? Infinitely
many, since we can always create more complex ones. However, typically
semantics will focus on <em>simple patterns</em> to assign basic
meanings, and complex meanings built from the combination of those. So
likely one would designate a finite or manageable set of structural
motifs that correspond to primitive semantic operations (like a basis of
meaning). Others might be combination of those.</p>
<p>For instance, in natural language semantics, one might think of <span
class="math inline"><em>δ</em></span> as combining an adjective and a
noun as we’ve said. Perhaps there’s a pattern that means “and” or “or”
by structure.</p>
<p>One can draw a parallel with <strong>combinatory logic</strong>: In
combinatory logic, the two basic combinators <span
class="math inline"><em>S</em></span> and <span
class="math inline"><em>K</em></span> (and maybe <span
class="math inline"><em>I</em></span>) have certain reduction patterns
(like <span
class="math inline"><em>K</em><em>x</em><em>y</em> = <em>x</em></span>,
<span
class="math inline"><em>S</em><em>x</em><em>y</em><em>z</em> = <em>x</em><em>z</em>(<em>y</em><em>z</em>)</span>)
which give them meaning in terms of lambda calculus. In our system, we
haven’t defined reduction or evaluation rules, it’s just a static
structure. But if one wanted to incorporate some logic, one could
specify certain <em>rewriting rules</em> (pattern rules) that might
capture how to interpret the structure. For example, if <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> was
intended to sometimes simplify, one could add rules like <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>X</em>)→</span>
something. But so far we’ve not done rewriting, just static
identity.</p>
<p>Let’s not go into rewriting; pattern-driven semantics can also mean
just <em>pattern recognition of specific substructures corresponds to a
concept</em>. For example, perhaps the pattern <span
class="math inline"><em>δ</em>(<em>X</em>,<em>δ</em>(<em>Y</em>,<em>Z</em>))</span>
might be interpreted in English as “X such-that Y of Z” or something
weird. Or <span
class="math inline"><em>δ</em>(<em>δ</em>(<em>X</em>,<em>Y</em>),<em>Z</em>)</span>
as “if X of Y then Z” depending on what X, Y, Z themselves are. It’s
quite open-ended.</p>
<p>The main philosophical point: In absence of intrinsic semantic, any
meaning must be externally imposed by noticing structural patterns. This
aligns with structuralist views that elements only have meaning in
context of structure (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=change,is%20subject%20both%20to%20history">Untitled
Document</a>). Here, patterns of <span
class="math inline"><em>δ</em></span> and <span
class="math inline"><em>I</em><sub>0</sub></span> would be that context.
It also aligns with phenomenology or post-structuralism to say
<em>meaning is not inherent, but emerges from relationships and
differences within a structured whole</em> (<a
href="https://en.wikipedia.org/wiki/Diff%C3%A9rance#:~:text=Diff%C3%A9rance%20is%20a%20French%20,relationship%20between%20text%20and%20meaning">Différance
- Wikipedia</a>).</p>
<p><strong>Axiom 4.1 (Structural Purity):</strong> We maintain Axiom 3.1
exactly. Actually, there’s no new formal axiom in version 4 beyond those
of version 3. The difference is conceptual: we now explicitly recognize
that <em>only structure exists in this theory; everything is <span
class="math inline"><em>δ</em></span> and <span
class="math inline"><em>I</em><sub>0</sub></span></em>.</p>
<p>We might add a meta-axiom or principle: <em>Semantic inertness.</em>
That is, <span class="math inline"><em>δ</em></span> and <span
class="math inline"><em>I</em><sub>0</sub></span> carry no predefined
meaning; meaning can only be assigned post hoc by interpretation of
structural patterns. It’s not an axiom of the algebra per se, but a
statement about how the formal system relates to meaning.</p>
<p><strong>Theorem 4.1 (Nothing New):</strong> Formally, <span
class="math inline">ℐ<sub>4</sub></span> as a set is identical to <span
class="math inline">ℐ<sub>3</sub></span> – because version 4 hasn’t
changed the generation rules. Thus all the results about unique
representation and complexity encoding from Version 3 still hold.
However, we now regard <span class="math inline">ℐ<sub>4</sub></span> as
a <em>blank canvas</em> for semantics, where any potential semantic
content must come from recognizing <em>patterns of <span
class="math inline"><em>δ</em></span> and <span
class="math inline"><em>I</em><sub>0</sub></span></em>.</p>
<p>We won’t repeat the proofs since they are identical. Instead, we
shift to philosophical discussion about this final state.</p>
<p>In Version 4, <strong>conceptual nonsense</strong> is not only
allowed, it is omnipresent unless filtered by semantics. The formal
system itself will never tell us “this structure is nonsense” – all
structures are equal citizens. The responsibility lies entirely with an
interpreter to either assign meaning or declare a given structure
meaningless (nonsense) relative to some interpretative scheme. For
example, one interpretative scheme might say: “Treat the first argument
of <span class="math inline"><em>δ</em></span> as a function and the
second as its input. If the first argument is not of a functional kind,
then the combination has no real-world meaning.” But this scheme is
external. Another scheme might be a <em>dialetheist logic</em> (one that
tolerates contradictions) that might even assign some “nonsense meaning”
to every term to keep it in the discourse.</p>
<p><strong>Pattern-driven semantics</strong> means, in effect: - There
is a (possibly partial) function <span class="math inline">[[⋅]]</span>
mapping certain ideas in <span class="math inline">ℐ<sub>4</sub></span>
to semantic values (which could be concepts, truth values, objects in
the world, etc). The domain of <span class="math inline">[[⋅]]</span>
might not be all of <span class="math inline">ℐ<sub>4</sub></span>, as
some structures may be left uninterpreted (considered nonsense or
irrelevant). - <span class="math inline">[[⋅]]</span> is defined by
recognizing patterns. For instance, one rule might say: for any idea of
the form <span
class="math inline"><em>δ</em>(<em>P</em>,<em>Q</em>)</span> where <span
class="math inline"><em>P</em></span> itself equals <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
(our earlier ‘A’), define <span
class="math inline">[[<em>δ</em>(<em>P</em>,<em>Q</em>)]] = ColorOf([[<em>Q</em>]])</span>
(if we assign that pattern meaning “color of X”). Another rule: if <span
class="math inline"><em>P</em> = <em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>I</em><sub>0</sub>)</span>
(our ‘B’), do something else, etc.</p>
<ul>
<li>This is akin to how in natural language semantics, one might parse a
sentence into a syntax tree and then interpret according to grammar
rules: those grammar rules are essentially pattern-driven semantics on
the tree structure of the sentence.</li>
</ul>
<p>Philosophically, one could say at this stage our formal theory
vindicates a <strong>structuralist ontology of ideas</strong>: an idea
<em>is</em> nothing but a structure (a position in a structure derived
from one primitive). There is no “essence” to ideas beyond that
structure. If two ideas have the same structure, they are the same idea.
If different, they are different. If we want to talk about their
meaning, we have to do so by referencing structure differences (like
“the idea that is one δ to the left vs one δ to the right etc”).</p>
<p>This can be tied to the notion of <strong>δ-based complexity</strong>
as a surrogate for meaning identity. For instance, maybe simpler ideas
(with smaller complexity measure) correspond to more fundamental
concepts and complex ones to more composite concepts. This might echo
Occam’s razor or minimal description length principle in knowledge
representation (prefer simpler structures as representing more general
or fundamental ideas, as they require fewer abstraction steps).</p>
<p><strong>Uniqueness of Abstraction Chains</strong> has a philosophical
implication: you can trace each idea back to the primitive in one way.
That could correspond to <em>each concept has a unique genealogy or
unique definition in terms of simpler concepts</em>. That is quite a
strong claim in conceptual analysis (in reality, often one concept can
be defined in multiple ways in words, but here formally, there’s one
formal definition path). This uniqueness could be interpreted as a kind
of guarantee of <em>non-ambiguity</em> in the conceptual system. If
there were multiple chains, that would be akin to conceptual ambiguity
or two different definitions that yield the same concept; we designed
that out by extensional identity.</p>
<p><strong>Deferred Semantic Equivalence:</strong> We have consistently
not identified any two structures as “the same” semantically unless they
are structurally the same. This means synonyms (two different structures
meaning the same thing) are not recognized formally. They would appear
as two distinct ideas. Only later an interpreter might say “hey, these
two different formal ideas actually mean the same real concept”. But
that would be an external equivalence, not in the formal core. For
instance, maybe <span
class="math inline"><em>δ</em>(<em>A</em>,<em>X</em>)</span> and <span
class="math inline"><em>δ</em>(<em>B</em>,<em>Y</em>)</span> might mean
the same concept “big vehicle” if <span
class="math inline"><em>A</em></span> and <span
class="math inline"><em>X</em></span> arrangements equate to “big car”
and <span class="math inline"><em>B</em></span> and <span
class="math inline"><em>Y</em></span> to “large automobile” – formally
different structures, semantically perhaps identical. Our theory would
count them as distinct ideas (because different structure), reflecting a
<em>skeptical, anti-Platonic stance</em>: it’s not assuming a
pre-existing identity beyond structure.</p>
<p>Now, one might worry: because we allowed nonsense, is the theory
<strong>inconsistent</strong> or trivial? There’s no notion of truth or
falsity here, just existence of ideas. So there’s no logical
inconsistency like a contradiction formula. It’s more like a
combinatorial algebra that is clearly consistent (it has models,
e.g. any term algebra model). So it’s fine. It’s just very
permissive.</p>
<p>We can now sum up a few key <strong>philosophical points</strong> in
the context of Version 4:</p>
<ul>
<li><p><strong>Structuralism</strong>: Our theory exemplifies
structuralism by showing that if you strip away semantic content, you
can still have a fully determined system of differences and relations
(here the relations given by the abstraction structure) (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=change,is%20subject%20both%20to%20history">Untitled
Document</a>). The actual “content” (meaning) can be later seen as
emerging from or being imposed on this structure. This echoes Saussure:
the value of each sign/idea is determined by what it is not (the
different position in the structure) (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=change,is%20subject%20both%20to%20history">Untitled
Document</a>). In our system, an idea is exactly characterized by how it
differs structurally from others (via <span
class="math inline"><em>δ</em></span> steps).</p></li>
<li><p><strong>Constructivism</strong>: We have constructed all ideas
from one base element, quite literally. There’s an analogy to how, say,
a child’s mind might bootstrap concepts: start from a primitive
undifferentiated sense experience (<span
class="math inline"><em>I</em><sub>0</sub></span>) and then apply mental
operations (abstractions, distinctions) repeatedly to build up a world
of concepts. Piaget might have talked about schemas that get
successively refined; here an “abstraction chain” could be seen as a
formal counterpart of that. Our theory suggests that <em>any idea can be
built stepwise</em>, aligning with a constructivist view that knowledge
is built and not just discovered fully formed.</p></li>
<li><p><strong>Phenomenology</strong>: In phenomenology, especially
Husserl, one distinguishes between the <em>noema</em> (the ideal content
of thought) and the <em>noesis</em> (the act of thinking). One could
argue our theory deals with noemata: the pure ideal structures of ideas,
without regard to their actual realization or how they present
themselves. Husserl would also bracket the actual existence and focus on
the structures of consciousness. Here we’ve bracketed semantics and just
have structures. One might also connect to <strong>bracketing</strong>:
we basically bracket meaning and just manipulate the “experience form”
(though that’s a stretch). Another angle:
<strong>foundationlessness</strong> akin to how phenomenology tries to
find the most primitive foundation of meaning (we started with one
primitive content and built everything – a highly rational
reconstruction of the content of thinking).</p></li>
<li><p><strong>Post-Structuralism</strong>: After establishing
structure, post-structuralists might point out that meaning is always in
flux and not pinned by structure alone. Our theory acknowledges that by
not even trying to pin meaning, but it sets the stage where meaning if
any will come from interplay of differences. Derrida’s
<em>différance</em> – difference and deferral – is reflected in how the
actual meaning of an idea is deferred (we never gave it, we said future
research or patterns will interpret it) and only differences (the
structural distinctions) are present now (<a
href="https://en.wikipedia.org/wiki/Diff%C3%A9rance#:~:text=Diff%C3%A9rance%20is%20a%20French%20,relationship%20between%20text%20and%20meaning">Différance
- Wikipedia</a>). The indefinite recombinability of <span
class="math inline"><em>δ</em></span> corresponds to an endless play of
signifiers. There’s no final signified in our system, only signifiers
all the way down (except <span
class="math inline"><em>I</em><sub>0</sub></span> which you might think
as a signified? But <span
class="math inline"><em>I</em><sub>0</sub></span> we treat just as an
empty mark, even that has no meaning by itself aside from being
base).</p></li>
<li><p><strong>Thought and Cognition</strong>: If we view <span
class="math inline">ℐ<sub>4</sub></span> as a model of the space of
thinkable concepts, an implication is that to think of something is to
carry out some combination of primitive mental operation (like <span
class="math inline"><em>δ</em></span>) on more basic thoughts, starting
from some primordial thought. This resonates with <strong>Jerry Fodor’s
Language of Thought (LOT)</strong> hypothesis that thinking involves a
combinatorial syntax in the head. In LOT, complex thoughts are built
from simpler ones via syntactic rules. Here <span
class="math inline"><em>δ</em></span> is our only syntactic combinator.
The uniqueness property might tie to Fodor’s idea that mental
representations have determinate structure (though synonyms show in
actual mind it might not be unique structure, but perhaps each person
has a canonical representation for a concept).</p></li>
<li><p><strong>Type theory comparison</strong>: In a typed lambda
calculus, one would disallow a lot of terms that our system allows. The
benefit of type theory is to avoid nonsense and ensure meaningful
composition, but the drawback is it restricts what can be expressed
(some valid concepts might require type circumvention or higher-order
types). Our approach is more like an untyped combinatory system which is
Turing-complete and very flexible, but also requires manual avoidance of
pitfalls. In a sense, our formal theory is <em>less safe</em> but
<em>more expressive</em> since it doesn’t enforce correctness at
generation time – a common trade-off between untyped and typed systems
(<a
href="https://carloangiuli.com/courses/b619-sp24/notes.pdf#:~:text=type%02checker%20may%20reject%20as%20nonsense,scoped">Principles
of Dependent Type Theory</a>). This can be justified if one’s aim is a
<strong>maximally general theory of ideas</strong> that doesn’t a priori
rule anything out, leaving it to additional principles (maybe an
‘analyst’ or ‘semantic filter’) to distinguish the sensible from the
nonsensical. It aligns with an <strong>academic skepticism</strong>:
rather than assume constraints (like “every idea must fall into a type
hierarchy or else it’s invalid”), we refrain from imposing that,
acknowledging that maybe our classifications might be wrong or too
limiting. This is somewhat like how Russell imposed type theory to avoid
paradox, while others (like Quine or some set theorists with AFA) later
said maybe we can relax that and handle the paradox differently. We are
choosing to allow potential paradox to even exist (though we didn’t
allow literal paradoxical loops, we did allow any non-paradoxical
nonsense).</p></li>
<li><p><strong>Self-reference and Reflection</strong>: Because our
system (especially from Version 3 onward) allows something to reference
itself or talk about itself structurally (like <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> is a form
of self-insertion), we can in principle represent statements about ideas
within the idea system. For example, one could imagine some pattern
corresponds to “concept X applied to itself”, which sometimes is
meaningful (like a function recursing on itself) and sometimes is
paradoxical (like a set containing itself). The theory does not break
from that, but a semantic interpretation might. This touches on
<strong>Russell’s paradox</strong>: if we had a semantics where <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> means “Y is
an element of set X”, then <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> would mean
“X is an element of itself”. Our theory would happily have <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> as an idea
for any X, but ZF set semantics would say that cannot be (except <span
class="math inline"><em>X</em></span> is not a set by foundation). So
our theory could embed discussion of non-well-founded sets if we
interpret it that way. It’s open enough.</p></li>
<li><p><strong>Conceptual Nonsense Justified</strong>: Why is allowing
nonsense a good thing? Philosophically, one might argue it parallels how
creative or out-of-the-box thinking works: by forming unusual
combinations (which might seem nonsensical at first) and sometimes those
yield new insights or metaphors. If the formal system forbids them, it
might blind itself to some innovations. There is also a more formal
reason: by including all combinations, our algebra is <em>complete</em>
in a generative sense and easier to analyze. If we had many restrictions
(like type rules), the theory’s combinatorics gets complicated and
possibly incomplete (some ideas can’t even be expressed to analyze). The
skeptical stance would be: let’s not rule out anything until absolutely
forced to (by contradiction or so). Wittgenstein in the
<em>Tractatus</em> tries to draw a limit to what can be said
meaningfully, calling beyond it nonsense (<a
href="https://plato.stanford.edu/entries/wittgenstein/#:~:text=words%2C%20to%20distinguish%20between%20sense,The%20conditions%20for%20a%20proposition%E2%80%99s">Ludwig
Wittgenstein (Stanford Encyclopedia of Philosophy)</a>), but then he
famously suggests that even the Tractatus’s own propositions are
nonsense to be thrown away after understanding (<a
href="https://plato.stanford.edu/entries/wittgenstein/#:~:text=nonsensical,what%20can%20only%20be%20shown">Ludwig
Wittgenstein (Stanford Encyclopedia of Philosophy)</a>). Similarly, our
theory suggests sometimes you have to traverse nonsense to see the
boundaries and maybe glean something (like how in a proof by
contradiction, you entertain nonsense to find out it’s false). The
formal allowance of nonsense means our system can talk about the
<em>form of nonsense itself</em>. Indeed, an idea that is semantically
gibberish still has a structure we can analyze. This might help in
studying things like paradoxes or even art (surreal combinations) in a
formal way.</p></li>
</ul>
<p>Finally, it’s important to re-iterate: <strong>Semantic Equivalence
is deferred</strong> in the sense that if two different structures
happen to mean the same in some external sense, we are not handling that
in the formal core. One day if we had a complete semantic theory, we
might impose an equivalence relation on <span
class="math inline">ℐ<sub>4</sub></span> to identify such synonyms. But
that would be a quotient of our structure, not inherent. For now, our
answer to “are ideas <span class="math inline"><em>X</em></span> and
<span class="math inline"><em>Y</em></span> the same concept?” is only
“are <span class="math inline"><em>X</em></span> and <span
class="math inline"><em>Y</em></span> structurally identical?” If not,
formally no; semantically, that question is outside of our current
formalism and to be answered by an interpretive mapping if one exists.
This ensures we do not mistakenly conflate concepts just because we
think they mean the same; we keep them separate to not lose information.
It’s a cautious approach: better to have duplicates than to merge
possibly distinct things incorrectly.</p>
<p>In conclusion of Version 4: We have achieved a <strong>minimal,
purely structural theory of ideas</strong>. It is <em>powerful enough to
represent any possible discrete idea structure</em>, yet <em>simple
enough to be specified with a single recursive rule</em>. The cost of
this generality is that <strong>meaning has completely vanished from the
formalism</strong>, except insofar as an external agent provides it via
pattern recognition. This is both a strength (we’ve eliminated any bias
or assumption about meaning – pure formal freedom) and a weakness (we
haven’t solved the problem of meaning at all, only postponed it).</p>
<h2 id="philosophical-meaning">Philosophical Meaning</h2>
<p>Having developed the formal theory through four versions, we now step
back to analyze its philosophical implications and underpinnings. The
theory was constructed with an eye on <strong>structuralism,
constructivism, phenomenology, and post-structuralism</strong>, and it
touches on deep questions about <strong>thought, cognition, reference,
and even nonsense</strong>. In this section, we will discuss these
aspects in turn, reflecting on what our theory says (and does not say)
about the nature of ideas.</p>
<h3 id="structuralism-and-the-primacy-of-structure">Structuralism and
the Primacy of Structure</h3>
<p>Our theory embodies the structuralist credo that <em>relations and
structure are fundamental</em>, while any notion of intrinsic essence is
dispensed with. Each idea is defined solely by its position in an
abstract structure (its unique abstraction chain or tree). This
resonates strongly with Ferdinand de Saussure’s view of language, where
signs have meaning only through their differences and relations in the
linguistic system (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=For%20Saussure%2C%20there%20are%20no,arbor%2Ftree%2Fsound%20image%2Fsignifier%20vs">Untitled
Document</a>) (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=change,is%20subject%20both%20to%20history">Untitled
Document</a>). In our case, an idea has no content besides “being the
<span class="math inline"><em>δ</em></span>-abstraction of some other
idea(s)” — it is entirely defined by how it differs from its
precursor(s). There are no “positive terms” or Platonic forms given
upfront; there is just the primitive <span
class="math inline"><em>I</em><sub>0</sub></span> (which itself is a
blank, signifying nothing except a starting point) and the network of
distinctions (applications of <span
class="math inline"><em>δ</em></span>). Thus, <em>ideas in our theory
are purely differential</em> (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=change,is%20subject%20both%20to%20history">Untitled
Document</a>): an idea is what it is by virtue of not being its
neighbors in the abstraction lattice.</p>
<p>This structuralist stance solves one problem while raising another.
It <em>solves</em> the problem of identity criteria for ideas: we have a
crisp rule — two ideas are identical iff their entire construction
structure is identical (an analogue of extensionality (<a
href="https://proofwiki.org/wiki/Axiom:Axiom_of_Extension#:~:text=The%20Axiom%20of%20Extension%20is,determined%20by%20its%20%205">Axiom:Axiom
of Extension - ProofWiki</a>)). This removes ambiguity: there is no need
to appeal to an ineffable essence or a semantic equality, which are
notoriously slippery. However, it raises the issue: if meaning is not
intrinsic, how do ideas acquire any meaning at all? Structuralists would
answer: meaning emerges from the structure of differences itself (<a
href="https://en.wikipedia.org/wiki/Diff%C3%A9rance#:~:text=Diff%C3%A9rance%20is%20a%20French%20,relationship%20between%20text%20and%20meaning">Différance
- Wikipedia</a>). In linguistics, the meaning of a word is determined by
its contextual relations (e.g., “day” means something partly because it
contrasts with “night”). In our framework, we can similarly say any
eventual meaning of an idea must come from its structural relations to
other ideas. For example, if one interpretation sees <span
class="math inline"><em>I</em><sub>0</sub></span> as “particular
instance” and <span class="math inline"><em>δ</em></span> as “conceptual
generalization”, then an idea <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>)</span>
might be interpreted as “a general concept abstracted from a particular
instance”. Its meaning (“some universal property”) is defined by its
structural position: it is a one-step generalization of the primitive.
Further abstractions (more <span class="math inline"><em>δ</em></span>
applications) yield more abstract concepts, and their meaning (in that
interpretation) is “even more general property”, etc. The <em>value</em>
of each idea is thus determined negatively, by how many abstraction
steps (and of what kind) separate it from concrete reality.</p>
<p>Our formal system enforces <strong>unique decomposition</strong>,
which aligns with a structuralist desire for a well-defined structure.
There is no ambiguity in how to analyze an idea: it has a <em>unique
syntax</em>, a single canonical decomposition (the abstraction
chain/tree). This echoes Lévi-Strauss’s structural anthropology approach
where each myth, for instance, has a single underlying structure to be
unveiled. Here each idea has a single underlying form (its derivation
from <span class="math inline"><em>I</em><sub>0</sub></span>). There is
no possibility of <em>structural ambiguity</em> or <em>synonymy</em> in
the formal sense. If two ideas appear synonymous in meaning, they will
appear as two distinct structures in our theory (because if they were
the same structure, they <em>are</em> identical formally). This formal
synonymy-avoidance is intentional: it reflects the structuralist
approach of refusing to collapse distinct elements just because of
subjective equivalence – instead, you keep them separate and study their
place in the system of relations (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=For%20Saussure%2C%20there%20are%20no,arbor%2Ftree%2Fsound%20image%2Fsignifier%20vs">Untitled
Document</a>). If indeed they have identical relations to all other
elements, then eventually structure would force them to coincide. But if
they differ in even one relational aspect, the theory keeps them
distinct. This is analogous to how extensional set theory distinguishes
sets that differ by one element in their membership, no matter if one
might think conceptually they represent “the same collection” – only
structure (membership) counts (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>).</p>
<h3 id="constructivism-and-the-genesis-of-ideas">Constructivism and the
Genesis of Ideas</h3>
<p>Our theory is explicitly constructivist: it shows how to
<em>build</em> the entire universe of discourse (all ideas) from a
single primitive via rule-governed operations. This is reminiscent of
Kantian or Piagetian constructivism, where complex knowledge is built up
from simpler components by cognitive acts. In fact, one can view <span
class="math inline"><em>δ</em></span> as a formal analog of a
<em>cognitive act of abstraction</em>. In Version 1, <span
class="math inline"><em>δ</em></span> was like a child’s first
abstracting of an object from experience (the very first concept from
the raw intuition). In Version 2, multiple modes of <span
class="math inline"><em>δ</em></span> correspond to different ways a
child (or a theorist) can abstract aspects of experience (color, shape,
function, etc.). In Version 3 and 4, the full generality allows
recombining abstracta in new ways, akin to how an adult mind can take
concepts and combine them to form new concepts, ad infinitum.</p>
<p>One might ask: does the theory imply that <em>all</em> ideas are
accessible by such stepwise construction? In other words, is it claiming
an almost <strong>genetic epistemology</strong> (in Piaget’s term) that
any high-level concept has a developmental sequence from more primitive
ones? Formally, yes: <em>for every idea in our “universe,” the theory
provides an explicit history tracing it back to <span
class="math inline"><em>I</em><sub>0</sub></span>.</em> This is a strong
constructivist claim. It resonates with the idea in philosophy that any
concept can (in principle) be analyzed into simpler concepts, ultimately
down to some primitives (empiricists might say down to sensory
primitives, rationalists to logical primitives). Our formal primitive
<span class="math inline"><em>I</em><sub>0</sub></span> is very abstract
– it could correspond to a raw <em>Ur</em>-experience or an initial
cognitive state. The theory doesn’t describe the nature of <span
class="math inline"><em>I</em><sub>0</sub></span> (it is contentless),
but it shows <em>if you grant me one primitive, I can construct a whole
edifice of thought</em>. This is akin to how set theory starts with ∅
and builds all mathematical objects, or how a constructive foundation
might start with basic percepts and build concepts.</p>
<p>Of course, there is a possible plurality of starting primitives (one
could start with more than one <span
class="math inline"><em>I</em><sub>0</sub></span> if one wanted to model
multiple unrelated base intuitions). Our theory chose one for
minimality. This choice reflects perhaps a monist or unified view of
origin: all ideas ultimately spring from a common source (maybe
analogous to a single “sensory continuum” or a single <em>Being</em> in
metaphysics). If one is more pluralist, one could extend the theory with
several disconnected <span
class="math inline"><em>I</em><sub>0</sub></span> constants to represent
independent root categories. But that would complicate the uniqueness
chain somewhat (there’d be multiple irreducibles). We stuck to one to
emphasize a unified origin. Interestingly, even if you had multiple
primitives, one could combine them under a higher abstraction anyway, so
one could still unify them at a higher level. In practice, one could
treat multiple primitives as all abstractions of an even more primitive
unspecified <span class="math inline"><em>I</em><sub>0</sub></span>
(just in different modes), thereby returning to the single source
paradigm. This aligns with constructivist attempts to find <em>the</em>
most primitive stuff of cognition (e.g., Piaget’s notion of
undifferentiated sensorimotor experience from which all distinctions are
gradually carved out).</p>
<p>Our formalism is neutral on <strong>empiricism vs
rationalism</strong> in origin of ideas. We have a primitive <span
class="math inline"><em>I</em><sub>0</sub></span> which could be thought
of as <em>given by experience</em> (empirical intuition) or as an <em>a
priori category</em> (if one thought <span
class="math inline"><em>I</em><sub>0</sub></span> represents, say, the
concept of “existence” or “the thing in itself” before any predicates).
The abstraction operation <span class="math inline"><em>δ</em></span>
can be seen as an <em>a priori mental operation</em> (Kantian category
perhaps) or as <em>generalization from instances</em> in empirical
learning. Version 2’s modes could align with <em>faculty psychology</em>
(different cognitive faculties abstract different qualities). However,
we carefully avoided building any empirical content into the formalism,
so it can equally model a purely formal construction (like building
mathematics from set theory) or a cognitive construction.</p>
<p>One might also connect this to <strong>intutionistic logic or
constructivist mathematics</strong>: we ensure each idea is
<em>constructible</em> effectively by a sequence of operations. There is
no non-constructible idea in the universe. This is a kind of <em>ideas
must be produced to exist</em> principle. It parallels how Brouwer
insisted that mathematical objects are constructed mentally, not
discovered ready-made. In our case, if you can’t point to a finite
sequence of abstractions yielding a purported idea, that idea simply
isn’t in our universe. This is a stringent ontological commitment:
<em>to be is to be constructed (from <span
class="math inline"><em>I</em><sub>0</sub></span>)</em>. There is no
magical infinite or self-subsistent idea. Even something like a
paradoxical concept (liar paradox) is constructed by combining simpler
linguistic ideas. We allow that construction, but it is still a
construction.</p>
<p>This helps avoid certain paradoxes by never introducing actual
infinite self-reference. As noted, our sets of ideas are well-founded
(no infinitely descending <span class="math inline"><em>δ</em></span>
chain), similar to how constructivist mathematics avoids actual
infinities or requires explicit procedures to approximate them. If one
wanted to consider an <em>infinite idea</em> (some idea that requires an
infinite sequence of abstractions), in our theory it simply does not
exist – unless we allow transfinite induction, which we didn’t. In
cognitive terms, this aligns with the notion that any actual human
concept must be formable in a finite mind through finite steps.</p>
<h3
id="phenomenology-the-structure-of-experience-without-content">Phenomenology:
The Structure of Experience Without Content</h3>
<p>Phenomenology, particularly Edmund Husserl’s, involves “bracketing”
the question of external reality to focus on the <em>structures of
consciousness</em> – the ways in which phenomena are given to us. Our
theory can be seen as performing a similar epoché (bracketing) with
respect to meaning: we have bracketed semantic content to focus on the
<em>formal structure of ideas themselves</em>. In phenomenology, one
analyzes the noetic-noematic structure – roughly, the mental act and its
intentional object structure – without assuming the object exists
externally. Here, we analyze how one idea can be intended by another
(like an abstraction intended by a prior idea) in pure structural
terms.</p>
<p>We might draw an analogy: let <span
class="math inline"><em>I</em><sub>0</sub></span> correspond to the
<strong>pre-reflective immediate experience</strong> (Husserl’s “hyle”
or raw content), and each application of <span
class="math inline"><em>δ</em></span> correspond to an act of
<em>ideation</em> or <em>abstraction</em> (Husserl’s “eidetic reduction”
maybe – moving from a particular to the idea of its essence). Then <span
class="math inline"><em>δ</em>(<em>I</em><sub>0</sub>)</span> would
correspond to capturing the essence (idea) of the primal experience,
<span
class="math inline"><em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>))</span>
capturing the essence of that essence (a higher-order concept), and so
forth. Husserl believed one could do iterated phenomenological
reductions to get at higher and higher-order categories of experience.
Our formalism parallels this iterability. It also parallels
<strong>philosophical dialectic</strong> (like Hegel’s progression: each
concept gives rise to a higher concept) albeit without Hegel’s semantic
richness.</p>
<p>One key phenomenological notion is that of
<strong>intentionality</strong>: every thought is about something. In
our structural theory, does an idea “point to” or “intend” something?
Not intrinsically – since we divorced semantics. However, we could say
that formally <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> is an idea
<em>directed at</em> <span class="math inline"><em>X</em></span> under
the mode/operator <span class="math inline"><em>F</em></span>. In that
sense, <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> has a kind
of intentional structure: <span class="math inline"><em>F</em></span> is
like the “conceptual lens” or noesis, and <span
class="math inline"><em>X</em></span> (the second argument) is like the
intentional object (the noema content) that <span
class="math inline"><em>F</em></span> is abstracting from. For example,
if <span class="math inline"><em>F</em></span> is an idea corresponding
to a property (pattern that means “redness”), and <span
class="math inline"><em>X</em></span> is an idea corresponding to an
object (“apple”), then <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span>
structurally resembles the intentional act “regarding X under aspect F”
(e.g. seeing the apple <em>as</em> red). This is not built into our
formal semantics, but structurally it matches how one might encode that
scenario. Thus our binary abstraction in Version 3/4 can be thought of
in intentional terms: the first component frames the act, the second is
the object of attention. This is deeply phenomenological – an idea is
not isolated but an act taking content and transforming it.</p>
<p>Our theory also allows <strong>self-intentional acts</strong> (<span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span>), which in
phenomenology might correspond to reflectivity: thinking of
<em>oneself</em> or a thought about itself (like self-consciousness).
Phenomenologists like Sartre and Brentano speak of consciousness being
aware of itself. In our formal world, <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> is a
perfectly valid idea which could be interpreted as “X applied to X” – if
X were a conscious state, that is the state taking itself as object. So
one could argue our framework is flexible enough to represent
self-consciousness structures, whereas more restrictive frameworks (like
simple type theory) would forbid self-application as ill-typed.</p>
<p><strong>Deferred semantic equivalence</strong> also resonates with
phenomenology’s suspension of the question of true reference. We do not
collapse ideas that might refer to the same object; we treat them as
separate noematic content as long as they appear differently to
consciousness. E.g., the morning star vs evening star example: two
different ideas (structures) might refer to the same Venus in reality,
but in our theory they remain distinct ideas (since structure differs).
This is analogous to Husserl’s idea that the noema (the perceived
object-as-structured-in-consciousness) is not identical to any external
object – it’s an intentional object, and different intentions (morning
star vs evening star) have different noemata even if the actual object
is the same planet. Only by an act of identification (outside the purely
structural description) do we say they coincide. Our theory leaves such
identifications to an external mapping (pattern semantics) rather than
building it in. Thus, it aligns with the idea that structure of
appearance is primary, truth correspondence is secondary.</p>
<h3
id="post-structuralism-and-the-freedom-and-danger-of-nonsense">Post-Structuralism
and the Freedom (and Danger) of Nonsense</h3>
<p>If structuralism is where we anchored our formal approach,
<strong>post-structuralism</strong> provides a critical perspective on
its limits. Post-structuralist thinkers like Jacques Derrida introduced
concepts such as <em>différance</em>, which emphasizes that meaning is
always deferred and mediated by endless play of signifiers, never fully
present in any structure (<a
href="https://en.wikipedia.org/wiki/Diff%C3%A9rance#:~:text=Diff%C3%A9rance%20is%20a%20French%20,relationship%20between%20text%20and%20meaning">Différance
- Wikipedia</a>). Our theory’s stance that <em>semantic equivalence is
deferred to future research</em> is quite in line with this: meaning is
always postponed – we don’t reach a final semantic truth in the
formalism, we only have signifier structures referencing other
signifiers. The primitive <span
class="math inline"><em>I</em><sub>0</sub></span> itself is an “empty
signifier” of sorts (it has no content, just a position as origin). Each
<span class="math inline"><em>δ</em></span> step produces a new
signifier out of old ones. In Derrida’s view, each signifier traces
differences from others and points to other signifiers in an endless
chain; similarly, each idea in our system points only to the prior ideas
that constitute it (its <span class="math inline"><em>δ</em></span>
components). There is no transcendental signified that stops the play –
indeed <span class="math inline"><em>I</em><sub>0</sub></span> could be
thought of as a candidate for “transcendental signified” (the thing that
just <em>is</em> without being defined), but in our theory <span
class="math inline"><em>I</em><sub>0</sub></span> has no meaning by
itself and, if interpreted, its meaning would likely be something like
“the ineffable substrate” – which is exactly not a fixed positive
content.</p>
<p>The <strong>generative complexity</strong> measure we introduced can
be seen as a kind of <em>trace</em> of an idea – a code that uniquely
marks its place in the system. In a metaphorical sense, that is like
Derrida’s idea that each presence bears the trace of the operations that
produced it (the context, the differences). Our <span
class="math inline"><em>δ</em></span>-codes are literally traces of how
the idea came to be (a history encoded). So every idea carries a
“heritage” or trace in its structure (there’s no featureless atomic
concept except <span class="math inline"><em>I</em><sub>0</sub></span>,
which itself can be seen as an empty trace).</p>
<p><strong>Abstraction chains are unique</strong> and <span
class="math inline"><em>δ</em></span>-complexity unique implies a very
rigid structure. Post-structuralists might challenge this rigidity by
arguing real meaning arises from ambiguity, slippage, and multiple
interpretations. Our formal system disallows ambiguity on a structural
level. But that doesn’t mean an external semantic can’t map two
different structures to the same meaning (which would be ambiguity in
interpretation). We just keep that ambiguity out of the formal
representation. This is consistent with a skeptical posture: we
acknowledge the possibility of synonymy but we don’t bake it into our
core assumptions — we want to <em>see it happen</em> if it does, not
assume it. In a way, we treat each apparent synonym as different until
proven identical, which is a safer approach to avoid conflating distinct
contexts. This could be seen as a formal analog of Derrida’s point that
no signifier has a single stable signified – here no idea has a single
given meaning; it’s always open.</p>
<p>Perhaps the most striking post-structural aspect is our
<strong>embrace of nonsense</strong>. We explicitly allow “conceptual
nonsense” in the formal system, which parallels the post-structuralists’
fascination with breaking boundaries of sense (e.g., Foucault’s analysis
of madness, or Derrida’s interest in the margins of sense). By
legitimizing nonsense structurally, we are recognizing that what is
considered nonsensical still has a form that can be studied. This is
akin to the mathematician’s approach (or Russell’s early approach) where
even paradoxical sets are at first given a logical form (“the set of all
sets that don’t contain themselves”) before deciding if it’s allowable.
We give nonsense a formal passport, so to speak.</p>
<p><strong>Why justify nonsense?</strong> One can philosophically argue
(as some postmodern or avant-garde thinkers do) that exploring nonsense
can lead to creative new sense. In literature and art, nonsense (think
of Lewis Carroll’s poems, or Dada art) is used to reveal hidden
assumptions of sense. In logic, allowing contradictions (in
paraconsistent logic) can sometimes yield systems that handle incomplete
information better. Our formal theory doesn’t resolve nonsense, but it
<em>keeps it in the realm of discussion.</em> If we had imposed a type
discipline or restrictions to preclude nonsense, we might inadvertently
exclude some edge cases of thought that, while “nonsensical” in one
semantic frame, could be reinterpreted in another. For example, the
expression “Color of Shape of X” might be nonsense in everyday
interpretation, but perhaps in a different interpretation it might mean
something (maybe “the shape has a color map”? – in a technical sense,
shapes can have color if we talk about graphical shape). By not
outlawing it, we leave room for later reinterpretation.</p>
<p>Another justification is <strong>completeness</strong>: the theory is
simpler and more elegant by not having a host of special cases that are
disallowed. We gave a simple generative rule and that’s it. This aligns
with Occam’s razor on theory-building: don’t complicate the generative
rules with semantic caveats. The cost is that the theory’s universe is
“too large” (includes absurdities), but we handle that not by altering
the theory, but by layering interpretations on top. This approach is
common in formal semantics of programming languages: the language might
allow you to write some crazy program, but the type system or runtime
catches it as an error – still, the syntax allowed it. We treat
semantics as a “runtime” or extra layer that can flag nonsense, but the
syntax (generation) is free. This separation of syntax (structure) and
semantics is a hallmark of formal approaches and matches the
<strong>academic skepticism</strong> of not conflating the two
prematurely.</p>
<p>In sum, our theory in its final form could be called
<strong>formalist structuralism</strong> tempered by a post-structural
openness. It gives an austere formal skeleton (structuralism), insists
everything must be built (constructivism), regards the structure as the
only thing given to consciousness (phenomenology), and yet acknowledges
that meaning might be an ever-elusive add-on and that even structurally
correct combinations can be senseless (post-structuralist insight into
the fragility of meaning). We have tried to clarify each claim we made
about the theory: none of them rely on unwarranted semantic leaps; they
are all justified by either induction, algebraic reasoning, or analogies
to known philosophical positions, as we have detailed.</p>
<h3
id="implications-for-thought-type-theory-and-self-reference">Implications
for Thought, Type Theory, and Self-Reference</h3>
<p>Before moving to applications, let’s reflect on a few specific
implications:</p>
<ul>
<li><p><strong>Thought and Cognition:</strong> If one tried to implement
this theory in a cognitive model, it implies that every concept in a
mind could be given a unique “concept ID” (like our complexity code)
that in principle could be computed by tracing how that concept was
formed from more primitive ones. This is interesting for AI and
cognitive science. It suggests, for instance, that if two people have
the “same concept,” they must have constructed it by an isomorphic chain
of abstractions (or else their concepts differ even if superficially
named the same). This is a strong claim about conceptual alignment. In
reality, two people might arrive at “concept of justice” via different
experiences and thus have slightly different conceptions. Our theory
would represent those as two distinct ideas (since the abstraction chain
differs if the paths of learning differ). This resonates with the idea
that no two individuals have <em>exactly</em> the same concept – there’s
always idiosyncratic differences in how they arrived at it, which might
reflect in subtle differences in understanding. However, it also offers
a way to measure similarity: you could measure overlap in their
abstraction chains or substructures.</p></li>
<li><p><strong>Comparison with Formal Theories
(Set/Type/Category):</strong> In ZF set theory, one also has a
cumulative hierarchy starting from ∅. Each set has a rank (like our
complexity measure) and unique construction tree (its membership tree)
assuming foundation. One difference is that in set theory, ∅ has a
definite interpretation (the empty collection), whereas our <span
class="math inline"><em>I</em><sub>0</sub></span> is not given a
meaning. Also, set theory has <em>unordered</em> combination (a set of
two elements doesn’t care order or repetition), whereas our <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> is ordered.
So our theory is more like building <em>tuples</em> or <em>directed
graphs</em>. It’s closer to an <strong>algebraic term system</strong> or
<strong>type-theoretic abstract syntax</strong>. In type theory,
normally you classify terms by types to avoid nonsense. We declined to
do that, meaning our theory is akin to a type theory with a single
universal type (like untyped lambda calculus or a dependently-typed
theory where every term inhabits the same universe type). This invites
the analogy: our <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> is similar
to <em>function application</em> in untyped lambda calculus, with <span
class="math inline"><em>F</em></span> playing role of function and <span
class="math inline"><em>X</em></span> argument. Indeed, untyped lambda
calculus can express anything computable but also has nonsensical
applications that don’t normalize, etc. We have not included lambda
abstraction (which would allow creating functions on the fly).
Interestingly, in Version 3, any idea can act as a function, but we did
not include an operation to create a new function from a pattern. We
only have the one combinator <span
class="math inline"><em>δ</em></span>. Lambda calculus has two
operations: function abstraction (λ) and application. We only have
application. Yet, our system can still mimic some combinator behavior by
treating certain ideas as combinators. In fact, the SK combinators of
logic are particular trees of application. It’s known that combinatory
logic (with suitable basic combinators) is universal. Our system didn’t
provide basic combinators explicitly, but one could consider certain
subtrees as analogous to S or K and see if our system, under some
reduction rules, would be computationally universal. We didn’t define
reduction though, so our system as is, is more like a combinatorial
space rather than a computational system.</p></li>
<li><p><strong>Self-Reference and Paradox:</strong> We allowed <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> and more
complex self-references. If one were to interpret <span
class="math inline"><em>δ</em>(<em>X</em>,<em>Y</em>)</span> as a
logical or set membership relation, <span
class="math inline"><em>δ</em>(<em>X</em>,<em>X</em>)</span> might
paradoxically assert something like “X is a member of itself” or “X
applies to X”. Russell’s paradox in set theory is <span
class="math inline">{<em>x</em>|<em>x</em> ∉ <em>x</em>}</span> which in
our notation might be an idea <span
class="math inline"><em>R</em></span> such that for any <span
class="math inline"><em>Y</em></span>, <span
class="math inline"><em>Y</em> ∈ <em>R</em></span> (if we had an
interpretation for membership) iff <span
class="math inline"><em>Y</em> ∉ <em>Y</em></span>. Expressing this
directly in our system would require some ability to talk about negation
or condition, which we haven’t got. But we can express <span
class="math inline"><em>Y</em> ∈ <em>Y</em></span> as <span
class="math inline"><em>δ</em>(<em>Y</em>,<em>Y</em>)</span> if we
interpret <span
class="math inline"><em>δ</em>(<em>P</em>,<em>Q</em>)</span> as <span
class="math inline"><em>P</em> ∈ <em>Q</em></span> or vice versa. Then
Russell’s set would correspond to <span
class="math inline">{<em>Y</em>|<em>δ</em>(<em>Y</em>,<em>Y</em>) is
false}</span>. Our theory doesn’t directly represent truth values or
comprehension axioms, so it doesn’t produce paradox by itself. It simply
allows the formation of the structure that would underlie the paradox if
an interpretation is given. For example, the structure for “the set of
all sets not containing themselves” might be something like an idea
<span class="math inline"><em>R</em></span> that satisfies a pattern
condition <span
class="math inline">∀<em>Y</em> : <em>δ</em>(<em>Y</em>,<em>R</em>) ↔︎ ¬<em>δ</em>(<em>Y</em>,<em>Y</em>)</span>.
That’s a second-order statement about our <span
class="math inline"><em>δ</em></span>. The theory can’t decide that –
it’s outside pure generation. So the <strong>presence of
self-referential structures</strong> in our theory does not equate to
inconsistency, because we have no rule that says some idea must or must
not contain itself. Everything is just a static object without truth
values. This again highlights the advantage of our <em>syntactic</em>
tolerance: many paradoxes in logic only arise when you impose certain
semantic or truth conditions; purely syntactic systems are often free of
those contradictions (though they might be incomplete or too
permissive).</p></li>
</ul>
<p>In closing this section: our rigorous, methodical tone in developing
the theory (with definitions, axioms, theorems) hopefully instills
confidence that even though the subject is abstract – “the formal theory
of ideas” – we have treated it with the same care as one would treat a
formal system in mathematics. We have not allowed ourselves unfounded
claims. If something is claimed (e.g., uniqueness of representation or
the ability to encode structure in a number), we either proved it or
cited known results (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>) (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=Freely%20generated%20algebraic%20structure%20over,a%20given%20signature">Term
algebra - Wikipedia</a>). Where we speculated (like philosophical
interpretations), we marked it as interpretation and not part of the
formal core (so the reader can separate the solid formal result from the
philosophical gloss).</p>
<p>The skepticism in tone also means we acknowledge limits: our theory
does <em>not</em> give semantic equivalence criteria – it leaves that
open. It does not ensure that formal nonsense can be made meaningful –
it only allows it to exist. It does not claim that human concepts
actually follow exactly these abstraction chains (that’s an empirical
question – we just provided a model). It does not claim that this
formalism by itself “explains understanding” – understanding would come
when a semantics is applied, which is beyond our current scope (we have
hints via pattern semantics how that might go).</p>
<p>In summary, philosophically our framework suggests: <strong>Ideas are
structure.</strong> Anything beyond that – like <em>truth, reference,
significance</em> – is a further layer to add, not guaranteed by
structure alone. This is a somewhat austere view (close to formalism and
structuralism), but it is also liberating: it means we can investigate
idea-combinatorics without getting bogged down in debates about what
those ideas inherently mean. Meaning can be considered later, once the
structural possibilities are mapped out. This separation of concerns is
akin to how in mathematics one might first classify all possible
solutions symbolically and later plug in interpretations to see which
solution makes sense in context.</p>
<h2 id="applications">Applications</h2>
<p>Having established our formal theory of ideas and examined its
philosophical significance, we now consider potential
<strong>applications</strong> across various domains. Although our
theory is abstract, its emphasis on unique structural representation and
generative complexity can be leveraged in fields ranging from knowledge
representation in AI to foundations of mathematics and logic. We discuss
a selection of applications below, highlighting how the core features of
the theory — <strong>structural uniqueness, δ-based complexity, and
allowance of all combinations</strong> — can be advantageous.</p>
<h3 id="knowledge-representation-and-ontologies">1. Knowledge
Representation and Ontologies</h3>
<p>In artificial intelligence and semantic web research, one often
builds <strong>ontologies</strong> or knowledge graphs where concepts
are related to each other. Our theory could provide a formal backbone
for such ontologies by treating each concept as an “idea” in the formal
sense, constructed from more primitive concepts. Because each idea in
our theory has a canonical representation (a unique abstraction chain or
tree), we could assign each concept a unique identifier based on that
structure (similar to a hash of its construction). This would prevent
the classic problem of <em>duplicate entries</em> in knowledge bases.
For example, if two researchers independently introduce the concept of
“binary tree of depth 3” in an ontology, and if both build it from the
same primitives (“binary tree” and “depth” and “3”) via the same
abstraction steps, our theory would identify them as the same formal
idea (their δ-complexity code would coincide). Conversely, if someone
defines “prime number” as “a number with exactly two divisors” and
another defines it as “a number not divisible by any smaller natural
except 1”, those are semantically equivalent but constructed via
different chains of simpler ideas (one via divisor-count property,
another via divisibility property). Our framework would keep them
distinct (two separate idea structures), alerting knowledge engineers
that there are <em>two formulations of prime</em> that need alignment.
Rather than assuming they are the same, the system would flag a
potential equivalence that a human or higher-level reasoning must
reconcile, thus avoiding unintended merging of distinct nodes. This is
an application of our <strong>structural identity criterion</strong> to
maintaining ontological rigor: concepts are merged only when their
defining construction is identical, otherwise they’re kept separate
(with maybe a link “these two are known to be equivalent” that can be
added manually or proven within a richer system).</p>
<p>The <strong>δ-based complexity measure</strong> can also inform
ontology management. It provides a natural notion of <em>conceptual
complexity</em>: an idea with a high δ-complexity (many abstraction
steps) is a very composite concept. This might correlate with how
difficult the concept is or how specialized. In knowledge bases, one
might want to ensure that more complex concepts are built from simpler
ones, akin to not introducing overly complex concepts before defining
the basics. Our theory inherently enforces that: you cannot have a
concept without all its parts being defined (since you need to construct
it from prior ideas). In an implemented system, one could use the
complexity ranking to, say, generate learning curricula (list concepts
from simplest to most complex) or to identify which fundamental concepts
a high-level concept depends on (by tracing its unique chain back to
primitives). This has echoes of <em>prerequisite graphs</em> in
education and could enhance intelligent tutoring systems: they would
know exactly which simpler ideas a student must grasp to understand a
given complex idea.</p>
<p>Another application in this area is in <strong>knowledge
integration</strong>: when combining two databases or ontologies, one
often faces the problem of aligning concepts. Using our approach, each
concept would carry its structural “DNA” (the δ-code). Concepts from
both sources that have matching codes can be automatically merged, those
with similar but not identical codes might be mapped via some
pattern-matching (e.g., find where one chain differs from the other and
that difference might correspond to a known synonym pattern). This is
like aligning parse trees in machine translation – here aligning concept
trees. The uniqueness property (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>) ensures that if they truly intended the same idea,
they will have isomorphic structure somewhere down the line (unless one
ontology was built with entirely different primitives, in which case
mapping the primitives is the first step).</p>
<h3 id="type-theory-and-programming-languages">2. Type Theory and
Programming Languages</h3>
<p>Our theory can be seen as providing a <em>universal type</em> (the
type of “idea”) with a single binary constructor. In programming
language terms, this is reminiscent of a universal data structure like
an S-expression or a JSON where everything is either an atom or a
pair/cons cell. Indeed, one can think of <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> as a
<em>cons cell</em> holding two pointers (to <span
class="math inline"><em>F</em></span> and <span
class="math inline"><em>X</em></span>). This suggests implementing a
<strong>knowledge programming language</strong> where all data are
represented in a uniform binary tree format (much like Lisp, where
everything is a list). The benefit of our approach over a naive Lisp
representation is the strong emphasis on canonical form. In Lisp, one
might accidentally have two equal lists that are structurally the same
but not recognized as the same object (unless interned). In our
approach, one could implement a <strong>unique idea interner</strong>
such that whenever a new idea is constructed, the system checks if an
identical structure was already built and if so, reuses it. This is akin
to how functional programming languages might memoize or use
hash-consing for tree nodes (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=As%20an%20example%20for%20unique,displaystyle">Term
algebra - Wikipedia</a>). Our theory assures that using hash-cons (hash
by the δ-code) yields a perfect sharing: no duplicates. This could
drastically save memory and improve logical comparison in a large
symbolic AI system.</p>
<p>In type theory specifically, while standard type theory would avoid
the “nonsense” terms by typing, there is a recent trend in
dependently-typed languages and theorem provers to allow more
expressions and then have propositions about them to constrain them. For
instance, one might embed an untyped expression language inside a type
theory and then state as a theorem that certain bad combinations are not
realizable. Our theory could be such an <em>embedded untyped core</em>
within a typed framework. The advantage is that the core is extremely
simple (just one type of node). One could then overlay a type system as
an extra predicate or layer, essentially filtering the <span
class="math inline">ℐ<sub>4</sub></span> universe to a subset of
“well-typed ideas”. This is similar to how in a logical framework like
Coq or Agda, one might have an inductive type of <em>terms</em> and then
a separate inductive relation of <em>well-formedness</em>. By separating
them, the metatheory can be simpler (e.g., proving strong normalization
of the well-formed sublanguage by induction on structure, etc., while
the larger space is there to consider ill-formed cases in proofs by
contradiction). Thus, our approach could inform the design of
<strong>meta logical frameworks</strong>.</p>
<p>Because <span class="math inline">ℐ<sub>4</sub></span> is essentially
the term algebra for one binary function symbol, it has the property of
being the <em>initial algebra</em> of that functor (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=signature%2C%20and%20this%20object%2C%20unique,5">Term
algebra - Wikipedia</a>). In category theory, initial objects have
unique arrows to any object satisfying the constraints. This means if
one defines any other data type by a similar recursion, there’s a unique
homomorphism from our idea algebra to that data type. In practical
terms, if you have a recursively defined data type in a programming
language (like a binary tree type of some sort), there is a unique way
to map an idea structure into that data type (preserving the abstraction
combination structure). This can be exploited for <strong>generic
programming</strong>: we can write generic functions by structural
recursion on <span class="math inline">ℐ<sub>4</sub></span> and then
apply them via the unique mapping to other more specialized types. This
is somewhat analogous to how one uses generic abstract syntax trees in
compilers and then translates them to machine-specific code. Our <span
class="math inline">ℐ<sub>4</sub></span> could be a lingua franca for
representing any inductively defined data. This touches on category
theory and <strong>universal properties</strong>: the initial algebra
property (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=signature%2C%20and%20this%20object%2C%20unique,5">Term
algebra - Wikipedia</a>) is a kind of universality. It implies that any
other algebra (any other idea-like structure) will receive a
homomorphism from ours. So our theory can simulate or embed many others
(with appropriate interpretation of the primitive and <span
class="math inline"><em>δ</em></span>).</p>
<h3 id="cognitive-science-and-conceptual-combination">3. Cognitive
Science and Conceptual Combination</h3>
<p>Human creativity often involves <strong>conceptual blending</strong>:
taking two ideas and combining them into a novel idea. Our Version 3 and
4 directly provide a formal model for conceptual combination: <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> is
essentially “idea <span class="math inline"><em>F</em></span> applied to
idea <span class="math inline"><em>X</em></span>”. Psychologically,
<span class="math inline"><em>F</em></span> might be a concept like an
adjective or modifier, and <span class="math inline"><em>X</em></span> a
base concept, resulting in a composite concept. For instance, <em>“pet
chicken”</em> or <em>“flying car”</em> are combinations of ideas that
might not have existed as single units before. We allowed even unusual
combinations (like <em>“car flying” as a concept, which might be
interpreted similarly to “flying car” or maybe nonsense, but it’s
</em>formed*). Cognitive science can use such a formal space to study
which combinations result in stable concepts (some combinations might
quickly become part of our conceptual repertoire) and which remain
marginal or nonsensical.</p>
<p>One could imagine a cognitive simulation where <span
class="math inline"><em>I</em><sub>0</sub></span> corresponds to a
sensory instance, and initial unary abstractions correspond to percepts
(like the concept “red” from seeing a red object). Multi-modal
abstraction corresponds to extracting different features (color, shape,
function, etc.) and binary abstraction corresponds to applying one
concept to another (treating one concept as modifying or relating to
another). The uniqueness of abstraction chains corresponds to a
cognitive hypothesis that each concept in a person’s mind has a unique
origin story — a path of associations or abstractions that led to it. It
might be challenging to validate, but it could impose useful structure
on models (preventing loops of concept definitions, etc.).</p>
<p>In computational creativity or generative AI, one might use our
framework to systematically generate new concept combinations. Because
nonsense is allowed, the system would generate many bizarre
combinations, but that’s acceptable in brainstorming phases. One then
needs a filtering mechanism (which could be learned or rule-based) to
pick combinations that are meaningful or useful. The formalism ensures
exhaustive coverage: <em>all</em> combinations of given building blocks
are considered (since we don’t rule any out). This is good for
completeness (not missing a potentially brilliant idea because it looked
odd initially).</p>
<p>In <strong>machine learning</strong>, there is interest in
<em>disentangled representations</em> and building complex features from
simpler ones. Our theory’s multi-modal abstraction (Version 2) is akin
to having separate factors of variation (like one mode for color, one
for shape, etc.), and then combining them gives a lattice of features.
If a neural network or other model could be constrained to follow such a
structure, it might learn more interpretable representations (each layer
or combination corresponding to an idea in this formal sense). Also, the
δ-complexity could serve as a regularizer: one might want to limit the
complexity of concepts a model forms to improve generalization (similar
to limiting the size of decision trees or depth of neural networks). In
our theory, complexity literally corresponds to size/depth of the idea’s
tree, so one could penalize overly deep constructions.</p>
<h3 id="mathematics-and-logic-foundational-perspectives">4. Mathematics
and Logic (Foundational Perspectives)</h3>
<p>Mathematically, <span class="math inline">ℐ<sub>4</sub></span> (the
set of all ideas) is a very simple set with a lot of structure: it’s the
free term algebra on one generator and one binary operation. This
structure is well-known in combinatorics: it is related to the set of
<em>full binary trees</em>. The number of distinct ideas with <span
class="math inline"><em>n</em></span> <span
class="math inline"><em>δ</em></span> nodes corresponds to the Catalan
numbers (if <span class="math inline"><em>I</em><sub>0</sub></span>
occurrences are not counted as nodes, just the internal <span
class="math inline"><em>δ</em></span> nodes). This means our theory has
a combinatorial enumeration: the count of ideas by complexity grows
catalanically (which is roughly exponential in <span
class="math inline"><em>n</em></span>). Knowing this allows theoretical
computer scientists or logicians to analyze complexity of algorithms
that might traverse the idea space. For instance, a brute-force search
through ideas up to complexity <span
class="math inline"><em>n</em></span> would be exponential in <span
class="math inline"><em>n</em></span>, which means we need smarter
strategies (like guided search or pruning using semantic criteria).</p>
<p>The <strong>unique representation</strong> also implies a form of the
<em>Church-Rosser property</em> (confluence) if we consider any sort of
reduction: although we did not define a reduction, if you imagine
simplifying idea expressions, the uniqueness of normal form is
guaranteed because we have essentially a normal form already (the given
tree). This is similar to how a term rewriting system that is
terminating and confluent yields unique normal forms (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=T%28X%29,nat%7D%7D.%20For%20example">Term
algebra - Wikipedia</a>). Here we have no rewriting, but if one defined
a rewriting on ideas (say some <span
class="math inline"><em>δ</em></span>-patterns rewrite to others), one
could attempt to prove confluence by leveraging the underlying unique
tree structure perhaps.</p>
<p>In foundations, one might compare this to <strong>Combinatory
Logic</strong> (CL) and <strong>Lambda Calculus</strong>. Combinatory
logic normally has two fixed combinators <span
class="math inline"><em>S</em></span> and <span
class="math inline"><em>K</em></span> from which all expressions can be
built. Our system doesn’t fix any combinators, but effectively <span
class="math inline"><em>I</em><sub>0</sub></span> plays a role similar
to an indeterminate, and <span class="math inline"><em>δ</em></span> is
function application. We could choose some specific <span
class="math inline"><em>I</em><sub>0</sub></span>-built structures to
act like <span class="math inline"><em>S</em></span> and <span
class="math inline"><em>K</em></span> (for example, treat <span
class="math inline"><em>I</em><sub>0</sub></span> as a variable <span
class="math inline"><em>x</em></span>, then <span
class="math inline"><em>K</em> = <em>δ</em>(<em>δ</em>(<em>I</em><sub>0</sub>,<em>I</em><sub>0</sub>),<em>x</em>)</span>,
purely structurally). It’s a bit contrived, but one could encode CL into
our system. If done, then <span class="math inline">ℐ<sub>4</sub></span>
would contain all combinatory logic terms as a subset. And we know CL is
Turing-complete, so <span class="math inline">ℐ<sub>4</sub></span> (with
an interpretation for <span class="math inline"><em>δ</em></span> as
function application) is at least able to represent any computable
function in principle. Without reduction rules, <span
class="math inline">ℐ<sub>4</sub></span> is just a static repository of
those expressions. But one could import reduction from CL or lambda
calculus, which then raises the question: does unique structure help in
reduction? Possibly, it means we don’t have to worry about α-conversion
(renaming of variables) because we have no named variables – all
structure is explicit. That’s a known perk of combinator or de-Bruijn
representations.</p>
<p><strong>Paraconsistent Logic and Dialetheism:</strong> In
philosophical logic, there are approaches that allow some contradictions
(true nonsense). One could use our framework to represent paradoxical
concepts (like the concept “this statement is false” as an idea
referencing itself via <span class="math inline"><em>δ</em></span> and
some “false” marker). While our base theory won’t resolve its truth
value, one can study the structure of the paradox within it. A
dialetheist might say that idea is both true and false; our system would
just host the idea as an object. Perhaps meta-logical reasoning could
assign it a truth value in a non-classical logic without causing the
system to explode (since the contradiction is isolated to that idea’s
evaluation, not an axiom of the whole system).</p>
<h3 id="linguistics-and-semiotics">5. Linguistics and Semiotics</h3>
<p>Our formal ideas are somewhat analogous to <em>syntactic trees</em>
in linguistics. One can imagine a simple language where the only syntax
is to take two expressions and form a new expression (binary
concatenation in some structured way). If we impose some interpretation,
<span class="math inline"><em>I</em><sub>0</sub></span> could be an
atomic word (say a proper noun or a basic object name), and <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> could
represent a phrase “F of X” or “X that is F” depending on order. Then
<span class="math inline">ℐ<sub>4</sub></span> describes all possible
phrases you can form using a recursive two-term composition. This is not
far from how actual grammar works (though natural languages have more
categories and restrictions). However, minimal recursion semantics or
combinatory categorial grammar sometimes represent meaning in terms of
function application and composition, which is akin to our <span
class="math inline"><em>δ</em></span>. In that sense, our system could
serve as a <strong>core syntax/semantics</strong> for a fragment of
language. The <em>unique chain</em> property corresponds to parse
unambiguity. Natural language is often <em>ambiguous</em> (different
parse trees yield same sentence string). Our formal language is not
ambiguous at the structural level because we don’t linearize it (it’s
inherently a tree form). If we linearized <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> as, say, “F
X”, then different trees could have the same linearization (“A (B C)” vs
“(A B) C” both linearize to “A B C” if parentheses are not shown). But
since our level of representation keeps the tree explicit, there’s no
ambiguity. In a computer implementation for natural language, one often
uses parse trees to avoid ambiguity too. So our structure could directly
represent meaning assembly without the need for disambiguation later
(all disambiguation is done by choosing one structure out of possible
ones).</p>
<p><strong>Semiotics:</strong> In semiotics (study of signs and
symbols), one might take a sign to consist of signifier and signified.
Our binary <span class="math inline"><em>δ</em></span> could possibly be
used to form a sign out of a signifier (as first part) and a signified
(second part). Then <span
class="math inline"><em>I</em><sub>0</sub></span> might represent some
primitive sign (maybe a pure form or pure content). Then an idea <span
class="math inline"><em>δ</em>(<em>S</em>,<em>C</em>)</span> would be a
full sign with form <span class="math inline"><em>S</em></span> and
content <span class="math inline"><em>C</em></span>. The theory would
then generate a whole system of signs. <em>Structural identity</em>
would mean two signs are identical only if both their signifier and
signified parts are identical (no two distinct signifiers with the same
content collapse into one sign in our formal sense – they’d be two signs
with maybe same meaning but different form, which is indeed how
semiotics would treat synonyms: different signifiers, considered
different signs despite same referent). So the formalism might model how
layers of meaning compose. For example, one could attach connotations as
further layers: <span
class="math inline"><em>δ</em>(Connotation,<em>δ</em>(wordForm,concept))</span>
as an idea structure meaning “the connotation attached to the sign
‘wordForm–concept’.” Many such patterns could capture semiotic
relationships.</p>
<h3 id="self-referential-systems-and-gödelian-phenomena">6.
Self-Referential Systems and Gödelian Phenomena</h3>
<p>One fascinating application is in analyzing self-referential systems.
Our theory can represent statements about statements (though it doesn’t
assign them truth). For example, consider Gödel numbering: in Gödel’s
incompleteness theorem, one encodes statements as numbers to let
statements refer to themselves. We could similarly encode each idea as a
number via δ-complexity (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>). Then for any idea <span
class="math inline"><em>X</em></span>, one can form an idea <span
class="math inline"><em>Y</em></span> that somehow asserts “X is not
provable” by using the number of <span
class="math inline"><em>X</em></span> within a formal provability
predicate pattern. Representing that fully would require expanding our
formalism with logical predicates, but as a skeleton, we have the
capability to place an idea’s own code inside itself or relate
structures with their codes. That is a step toward representing
<strong>Gödel sentences</strong>. The uniqueness of coding (injectivity)
is crucial — it’s exactly Gödel’s condition that coding be unique and
mechanizable (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>). Our δ-based complexity
provides a unique code and is effectively computable (just count and
assign indices). This means we have the raw material to import something
like Gödel numbering scheme where each idea “knows” its number or we can
construct an idea that contains the code of another (which is akin to
Gödel’s diagonal lemma construction).</p>
<p>While this is speculative, it indicates that our system, while
simple, is rich enough to express the kind of self-reference at the
heart of Gödel’s theorem, or Tarski’s undefinability theorem, etc., once
augmented with the notion of a truth predicate or provability predicate
as additional structure on top of <span
class="math inline">ℐ<sub>4</sub></span>. The benefit is that since each
idea is uniquely identified by a number, we avoid issues of different
expressions referring to the same statement — each statement (idea) is
canonical, so self-reference can be pinpointed exactly.</p>
<p>One could even attempt an application in
<strong>metaprogramming</strong> or <strong>reflection</strong> in
computer science: languages that have reflective capabilities (able to
manipulate their own code as data) often need to ensure the code has a
representation accessible at runtime. Our idea-codes and idea-structures
provide a uniform way to represent code within code. A Lisp program can
quote itself; similarly, in our framework, an idea can incorporate a
sub-idea that is basically a representation of another idea. Because
everything is an idea, quoting is just using the same structure at a
higher level. This might simplify designing reflective systems because
you don’t need a separate metalanguage — the object language is its own
metalanguage to some extent (like Lisp homoiconicity). Our <span
class="math inline"><em>I</em><sub>0</sub></span> and <span
class="math inline"><em>δ</em></span> are so general that an idea can
encode an interpreter for the idea system itself (that’s essentially
what a Gödelian self-reference does).</p>
<p><strong>Conclusion of Applications:</strong> The formal theory we
proposed might seem esoteric, but its implications and uses span many
fields. The key strengths that enable these applications are:</p>
<ul>
<li><p><strong>Universality of form:</strong> Everything is built from
one operation, making it a convenient <em>universal format</em> for
interchange or meta-representation.</p></li>
<li><p><strong>Canonical representation:</strong> Ideas have a unique
structure, which aids in identity checking, merging of data, and
eliminating ambiguity. This is especially useful in distributed systems,
knowledge graphs, and any scenario where you need a single source of
truth for a concept (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>).</p></li>
<li><p><strong>Hierarchical construction:</strong> The partial order by
complexity provides a natural hierarchy (which can be used in learning
sequences, ontology layering, etc.). It’s analogous to how more complex
theorems in math build on simpler ones; here more complex ideas build on
simpler ones.</p></li>
<li><p><strong>Allowance of all combinations:</strong> This maximizes
creativity and completeness, as noted in AI brainstorming or theoretical
search spaces. We don’t miss possibilities by imposing premature
restrictions.</p></li>
<li><p><strong>Simplicity of the formalism:</strong> Only one kind of
building block means implementations can be streamlined (one data type
to handle, one recursion to optimize). It is language-agnostic; it could
underlie very different semantic systems with the same structural
engine.</p></li>
</ul>
<p>Of course, when moving to practice, one would have to implement
additional layers (for example, a type-checker on top, or a semantic
evaluator, or domain-specific primitives akin to an expanding <span
class="math inline"><em>I</em><sub>0</sub></span> into many base
symbols). But the core could remain the same, which suggests our theory
might serve as a <strong>unifying foundation</strong> beneath disparate
systems. Similar ambitions have been seen in projects like <em>universal
knowledge representation languages</em> or <em>general logics</em>, and
our approach is in that spirit but boiled down to the absolute
minimalist core.</p>
<h2 id="conclusion">Conclusion</h2>
<p>We have presented a comprehensive development of a <strong>formal
theory of ideas</strong> that begins from a single primitive and reaches
a system of <strong>minimal binary abstraction</strong>. The journey
went through four versions, each adding expressive power while
preserving the central principle that <strong>only structural form
determines identity</strong> and meaning is not hard-wired. We summarize
the key accomplishments and insights of this work as follows:</p>
<ul>
<li><p><strong>Formal Framework:</strong> We defined a rigorous formal
system in which every idea is inductively generated from one primitive
<span class="math inline"><em>I</em><sub>0</sub></span> via the
abstraction operation <span class="math inline"><em>δ</em></span>. In
Version 1 and 2 (unary abstractions), ideas are organized in a hierarchy
akin to a tree of generalizations. In Version 3 (binary abstraction), we
allowed ideas to be <em>combined</em>, treating one idea as a function
or modifier of another. Finally, Version 4 distilled the system to its
simplest ingredients: one constant and one binary constructor, with no
further structure except that which emerges from their
combinations.</p></li>
<li><p><strong>Uniqueness of Abstraction Chains:</strong> A fundamental
theorem throughout was that each idea has a <strong>unique abstraction
chain/tree</strong> leading back to the primitive (Theorems 1.1, 2.1,
3.1). This gives our theory a property of <strong>unique
decomposition</strong> (no ambiguity in how an idea is built) and
mirrors extensionality in set theory (<a
href="https://proofwiki.org/wiki/Axiom:Axiom_of_Extension#:~:text=The%20Axiom%20of%20Extension%20is,determined%20by%20its%20%205">Axiom:Axiom
of Extension - ProofWiki</a>). Conceptually, this means every idea can
be seen as having a <em>unique definition</em> in terms of simpler
ideas. This uniqueness was not assumed but proved from the injectivity
of the abstraction operations (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>). It is a distinguishing feature compared to many
semantic networks or type systems, where sometimes multiple derivations
can lead to the same concept – our formal system avoids that by
design.</p></li>
<li><p><strong>δ-Based Complexity:</strong> We introduced a measure of
<strong>complexity</strong> for ideas based on the number and pattern of
<span class="math inline"><em>δ</em></span> operations required to
construct them. We proved that this δ-based complexity (essentially a
Gödel-like encoding (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>) of the idea’s structure) is
<strong>injective</strong>, i.e., it uniquely determines the idea’s
structure (Corollaries 1.2, 2.2, 3.2). Practically, this gives each idea
a sort of “fingerprint” or canonical name (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>). In applications, this can be
used as a hash or unique ID for concepts. The injectivity relies on
foundational mathematical facts like unique prime factorization and the
properties of pairing functions (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=As%20an%20example%20for%20unique,displaystyle">Term
algebra - Wikipedia</a>), which we leveraged in our proofs.</p></li>
<li><p><strong>Allowance of Conceptual Nonsense:</strong> We explicitly
<em>allow all syntactically well-formed ideas</em>, including those that
might be considered meaningless or contradictory under some
interpretation. This is not a bug but a feature: the theory does not
enforce semantic constraints at the structural level. We justified that
including “nonsensical” structures is harmless to the formal consistency
(since no semantics means no formal contradiction can arise from them)
and is in fact beneficial for <strong>completeness</strong> and
<strong>flexibility</strong>. It means the theory’s universe of ideas is
closed under the abstraction operation in the fullest sense – it’s an
algebraic closure. We discussed how any semantic filtering (to rule out
nonsense) can be applied <em>after</em> the fact, as an additional
layer, without needing to mutilate the formal generative capacity. This
design choice is aligned with practices in formal logic (e.g., the
principle that syntax is independent of semantics, and one can study
well-formed formulas even if they might be unsatisfiable or meaningless
in some model).</p></li>
<li><p><strong>Philosophical Alignment:</strong> We examined how our
formal theory connects to and illuminates various philosophical
perspectives:</p>
<ul>
<li>It embodies <strong>structuralism</strong> by making structure the
sole criterion for identity and by deferring meaning to relations (<a
href="https://www.webpages.uidaho.edu/~sflores/NotesOnSaussure.html#:~:text=change,is%20subject%20both%20to%20history">Untitled
Document</a>).</li>
<li>It resonates with <strong>constructivism</strong> in that all ideas
are built, not innate; it provides a clear constructive genesis for
every concept.</li>
<li>It parallels <strong>phenomenological bracketing</strong> by
focusing on the form of ideas (noema) rather than their external
reference, and it allows intentional structures (with <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> viewed as
an idea directed at <span class="math inline"><em>X</em></span> under
mode <span class="math inline"><em>F</em></span>).</li>
<li>It accommodates <strong>post-structuralist insights</strong> by
acknowledging the endless deferral of semantic resolution and by
including even disruptive elements (nonsense, self-reference) in the
discourse rather than excluding them.</li>
</ul>
<p>We maintained an academically skeptical tone, avoiding unwarranted
leaps. Every formal claim was supported by proof or reference to
established results, and speculative interpretations were clearly
delineated as such.</p></li>
<li><p><strong>Applications and Utility:</strong> We explored a range of
potential applications:</p>
<ul>
<li>In <strong>knowledge representation</strong>, our theory can serve
as a backbone for unique concept identifiers and dependency graphs (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>).</li>
<li>In <strong>type theory and programming languages</strong>, it offers
a universal untyped structure that can underlie typed systems or serve
as a medium for generic programming.</li>
<li>For <strong>cognitive science</strong>, it provides a model for
concept composition and a measure of conceptual complexity that could
correlate with learnability or cognitive load.</li>
<li>In <strong>logical foundations</strong>, it provides a simple domain
in which to encode self-referential statements or perform Gödel
numbering (<a
href="https://not-a-number.io/2024/understanding-kolmogorov-complexity/#:~:text=Kolmogorov%20complexity%20is%20about%20the,uncomputability%20aspect%20of%20Kolmogorov%20complexity">Understanding
Kolmogorov Complexity | Not a Number</a>), and its uniqueness property
can simplify meta-logical arguments (no need to consider multiple
representations of the same formula).</li>
<li>In <strong>semantics and linguistics</strong>, it gives a formal way
to represent possible compound meanings and is analogous to parse trees
or sign structures, with a clear separation of concerns between syntax
(structure) and semantics (pattern-based interpretation).</li>
</ul>
<p>These applications demonstrate that the theory is not merely an
abstract exercise; it has the potential to inform practical systems and
cross-pollinate with other disciplines. The emphasis on canonical form
and unique identifiers for ideas is especially timely in an era of
distributed information systems and AI knowledge graphs where aligning
concepts is a major challenge (<a
href="https://math.stackexchange.com/questions/400004/question-regarding-the-axiom-of-extensionality#:~:text=,">set
theory - Question Regarding the Axiom of Extensionality - Mathematics
Stack Exchange</a>).</p></li>
<li><p><strong>Limitations and Future Work:</strong> We are aware that
our theory, by itself, does not solve the problem of <strong>semantic
equivalence</strong> or <strong>truth</strong>. It intentionally
sidesteps those, focusing on generative structure. Future research is
needed to investigate how best to impose or learn semantic constraints
on the <span class="math inline">ℐ<sub>4</sub></span> universe. One
approach could be to define an interpretation function <span
class="math inline"><em>I</em> : ℐ<sub>4</sub> → <em>D</em></span> into
some domain of meanings and then characterize which ideas map to
equivalent elements of <span class="math inline"><em>D</em></span>.
Another approach is to extend the formal system with <em>types or
predicates</em> that restrict <span
class="math inline"><em>δ</em></span>’s application in various contexts
(essentially building typed or sorted versions of the theory). We
deferred this to keep the core minimal, but it is a promising direction
to obtain the <em>best of both worlds</em>: a universal idea
representation plus the ability to carve out, say, a subalgebra of
“well-typed ideas” that correspond to meaningful expressions in some
domain.</p></li>
</ul>
<p>Another area for future work is exploring <strong>variations of the
abstraction operator</strong>. We considered one binary operator. What
if we introduced a second binary operator (say <span
class="math inline"><em>δ</em><sub>1</sub></span> and <span
class="math inline"><em>δ</em><sub>2</sub></span>) or made <span
class="math inline"><em>δ</em></span> associative or commutative via
axioms? That would radically change the structure (e.g., commutativity
would identify <span
class="math inline"><em>δ</em>(<em>F</em>,<em>X</em>)</span> with <span
class="math inline"><em>δ</em>(<em>X</em>,<em>F</em>)</span>, losing
uniqueness unless additional bookkeeping is added). Our current stance
is that adding algebraic identifications like commutativity would break
uniqueness (two different trees could then represent the same
equivalence class). So if one needs commutativity (for example, in an
idea of an <em>unordered combination</em>), one should enforce it at
semantic level or by sorting some substructures. These are design
choices that depend on the application domain.</p>
<p>Finally, there is scope to implement this theory in a
<strong>software prototype</strong> (perhaps as a knowledge base or
theorem prover’s kernel) to test its practicality. Would the δ-codes be
manageable in size? How would one perform pattern-driven semantic
interpretation effectively? These engineering questions can be addressed
with experiments. There is optimism that modern computing can handle
surprisingly large symbolic structures and that hashing or interning
techniques (<a
href="https://en.wikipedia.org/wiki/Term_algebra#:~:text=As%20an%20example%20for%20unique,displaystyle">Term
algebra - Wikipedia</a>) can mitigate the exponential blow-up for
moderate depths by reusing shared sub-structures.</p>
<p><strong>Concluding Thoughts:</strong> By reducing the theory of ideas
to a single primitive and a single mode of combination, we have exposed
the <strong>essential skeleton</strong> of conceptual structure. This
skeleton, albeit abstract, is sturdy: it can carry the weight of complex
thoughts without bending, because it carefully guarantees uniqueness and
consistency at every joint (each abstraction step is invertible and
distinct). In doing so, we have also avoided painting the skeleton with
any particular color of semantics — that is left to future anatomists of
meaning. As a result, the theory is <em>general</em>: it does not favor
any particular domain of ideas (mathematical, physical, metaphysical,
etc.), and it sets a neutral stage on which any semantic play can be
performed, with the assurance that the underlying scaffolding will keep
the actors (the ideas) well-organized and unambiguous.</p>
<p>In a way, this work is an argument that <strong>the form of thought
can be studied independently of its content</strong>, and that such
study yields dividends in clarity and organization. Just as algebra
reveals structure common to many mathematical systems by stripping away
specifics, our formal theory reveals a common structural core beneath
how ideas can relate to each other. We have only one basic relation:
“being an abstraction of / applied to”. And yet, from that relation
unfolds an infinite tapestry of possible constructs. By examining the
properties of that relation (associativity, uniqueness, etc.), we
derived a coherent picture of that tapestry.</p>
<p>To return to the title: <em>“Structure, Uniqueness, and Generative
Complexity from a Single Primitive”</em> – we indeed found structure (a
whole algebra of ideas), uniqueness (each idea one-of-a-kind by
construction), and a measure of generative complexity
(δ-length/encoding) all stemming from the simplest of beginnings. This
suggests a philosophical point: <strong>perhaps complexity in the realm
of ideas does not require complex primitives, only powerful ways to
combine</strong>. A lone primitive with a binary nexus was sufficient to
engender unlimited complexity. This echoes how a binary operation (like
a NAND gate) is enough to build any computation in computer science (<a
href="https://en.wikipedia.org/wiki/Abstract_nonsense#:~:text=Roughly%20speaking%2C%20category%20theory%20is,53%20was%20critical%20of%20this">Abstract
nonsense - Wikipedia</a>), or how two simple rules can generate a
fractal of infinite detail. Our theory shows something analogous for the
space of concepts.</p>
<p>In conclusion, we have not solved the mystery of meaning, but we have
provided a stable formal platform on which that mystery can be explored.
The <strong>formal theory of ideas</strong> laid out here is intended as
a foundation — both a rigorous mathematical foundation (in the sense of
providing a well-defined domain for reasoning about concepts) and a
conceptual foundation (clarifying how complex ideas relate to simpler
ones in principle). Future explorations will build upon this foundation,
adding layers of interpretation, integrating it with empirical
knowledge, and potentially refining the operations (for instance,
introducing new primitives or quotienting by equivalences as needed).
But the hope is that by having identified a <strong>minimal, sound
core</strong>, all such extensions will be more principled and easier to
manage, much as a clear skeleton aids the growth and adaptation of a
living organism.</p>
<p>The paper combined formal development with philosophical and
practical reflections, maintaining a rigorous approach throughout. By
doing so, we aimed to satisfy both the <strong>academic skeptic</strong>
(who demands clarity, proofs, and no hand-waving) and the
<strong>visionary</strong> (who sees the big picture of how this could
influence understanding of ideas in various fields). The end result is a
theory that is at once <strong>simple and far-reaching</strong>. We
invite others to test it, apply it, and even critique it — for in a
truly structuralist spirit, it is through the network of differences
(other theories, interpretations, implementations) that the significance
of this theory of ideas will be fully realized.</p>
</body>
</html>
